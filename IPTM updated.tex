\documentclass[a4paper]{article}
\usepackage{geometry}
\geometry{
	a4paper,
	total={170mm,257mm},
	left=27mm,
	right=30mm,
	top=30mm,
	bottom= 30mm
}
\usepackage{tabu}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{longtable}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{tikz}
\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
		\node[shape=circle,draw,inner sep=0.5pt] (char) {#1};}}
\usetikzlibrary{fit,positioning}
\usepackage{authblk}
\usepackage{natbib}
\usepackage[algo2e]{algorithm2e}
\usepackage{algorithmic}  
\usepackage{algorithm}
\usepackage{comment}
\usepackage{array}% http://ctan.org/pkg/array
\makeatletter
\g@addto@macro{\endtabular}{\rowfont{}}% Clear row font
\makeatother
\newcommand{\rowfonttype}{}% Current row font
\newcommand{\rowfont}[1]{% Set current row font
	\gdef\rowfonttype{#1}#1%
}
\newcolumntype{L}{>{\rowfonttype}l}
\title{Interaction-Partitioned Topic Models (IPTM) \\using a Point Process Approach}
%\author{Bomin Kim}

\author[1]{Bomin Kim}
\author[1]{Bruce Desmarais}
\author[2,3]{Hanna Wallach}
\affil[1]{Pennsylvania State University}
\affil[2]{Microsoft Research NYC}
\affil[3]{University of Massachusetts Amherst}

\begin{document}
\maketitle
\section{Ideas}
Current CPME model does not involve any of temporal component, which plays a key role in email interactions. Intuitively, past interaction behaviors significantly influence future ones; for example, if an actor $i$ sent an email to actor $j$, then $j$ is highly likely to send an email back to $i$ as a response (i.e. reciprocity). Moreover, the recency and frequency of past interactions can also be considered to effectively predict future interactions. Thus, as an exploratory data analysis, point process model for directional interaction is applied to the North Carolina email data. Starting from the existing framework focused on the analysis of content-partitioned subnetworks, I would suggest an extended approach to analyze the data using the timestamps in the email, aiming to develop a joint dynamic or longitudinal model of text-valued ties.\\ \newline
 CPME model is a Bayesian framework using two well-known methods: Latent Dirichlet Allocation (LDA) and Latent Space Model (LSM). Basically, existence of edge depends on topic assignment $k$ (LDA) and its corresponding interaction pattern c. Each topic $k=1,…,K$ has one interaction pattern c=1,…,C, and each interaction pattern posits unique latent space (LSM), thus generating $A\times A$ matrix of probabilities $P^{(c)}$ that a message author
a will include recipient $r$ on the message, given that it is about
a topic in cluster $c$.  Incorporating point process approach, now assume that under each interaction pattern, we have $A\times A$ matrix of stochastic intensities at time $t$, $\boldsymbol{\lambda}^{(c)}(t)$, which depends on $\boldsymbol{x}^{(c)}_t(i, j)$, the history of interaction between the sender and receiver corresponding to the interaction pattern $c$. We will refer this as  interaction-partitioned topic models (IPTM). 
\section{IPTM Model}
In this section, we introduce multiplicative Cox regression model for the edge formation process in a longitudinal communication network. For concreteness, we frame our discussion of this model in terms of email data, although it is generally applicable to any similarly-structured communication data.
\subsection{Point Process Framework}
A single email, indexed by $d$, is represented by a set of tokens $w^{(d)} = \{w^{(d)}_m \}_{m=1}^{M^{(d)}}$ that comprise the
text of that email, an integer $i^{(d)} \in \{1,...,A\}$ indicating the identity of that email’s sender, an integer $j^{(d)} \in \{1,...,A\}$ indicating the identity of that email’s receiver, and an integer $t^{(d)} \in [0, T]$ indicating the (unix time-based) timestamp of that email. To capture the relationship between the interaction patterns expressed in an email and that email’s recipients, documents that share the interaction pattern $c$ are associated with an $A\times A$ matrix of $\boldsymbol{\lambda}^{(c)}(t)=\{\{\lambda^{(c)}_{ij}(t)\}_{i=1}^{A}\}_{j=1}^{A}$, the stochastic intensity where $\lambda^{(c)}_{ij}(t)dt$=P\{for interaction pattern $c$, $i\rightarrow j$ occurs in time interval $[t, t+dt)\}$. We will model the counting process $\mathbf{N}^{(d|c)}(t)$ through $\boldsymbol{\lambda}^{(c)}(t)$ using a version of the Cox proportional intensity model, where $N_{ij}^{(d|c)}(t)$ denotes the number of edges (emails) for document $d$ from actor $i$ to actor $j$ up to time $t$ (from the starting point 0) given that the document corresponds to interaction pattern $c$. Since this counting proess $\mathbf{N}$ is document-based, each element is either 0 or 1, and only one element of the matrix is 1 while all the rests are 0 (assuming no multicast). \\ \newline Combining the individual counting processes of all potential edges,  $\mathbf{N}^{(d|c)}(t)$ is the multivariate counting process with $\mathbf{N}^{(d|c)}(t)=(N^{(d|c)}_{ij}(t): i, j \in {1, ..., A}, i \neq j)$. Here we make no assumption about the independence of individual edge counting process. As in \cite{Vu2011}, we model the multivariate counting process via Doob-Meyer decomposition:
\begin{equation}
\mathbf{N}^{(d|c)}(t)=\int_0^t\boldsymbol{\lambda}^{(c)}(s)ds + \mathbf{M}(t)
\end{equation}
where essentially $\boldsymbol{\lambda}^{(c)}(t)$ and $\mathbf{M}(t)$ may be viewed as the (deterministic) signal and (martingale) noise, respectively.\\ \newline
Following the multiplicative Cox model of the intensity process $\boldsymbol{\lambda}^{(c)}(t)$ given $\boldsymbol{H}^{(c)}_{t-}$, the entire past of the network corresponding to the interaction pattern $c$ up to but not including time $t$, we consider for each potential directed edge $(i, j)$ the intensity forms:
\begin{equation}
\lambda^{(c)}_{ij}(t|\boldsymbol{H}^{(c)}_{t-})=\lambda_0\cdot \mbox{exp}\Big\{\boldsymbol{\beta}^{(c)T}\boldsymbol{x}^{(c)}_t(i, j)\Big\}\cdot 1\{j \in \mathcal{A}^{(c)}\}
\end{equation}
where $\lambda_0$ is the common baseline hazards for the overall interaction, $\boldsymbol{\beta}^{(c)}$ is an unknown vector of coefficients in $\boldsymbol{R}^{p}$, $\boldsymbol{x}^{(c)}_t(i, j)$ is a vector of $p$ statistics for directed edge $(i, j)$ constructed based on
$\boldsymbol{H}^{(c)}_{t-}$, and $\mathcal{A}^{(c)}$ is the predictable receiver set of sender $i$ corresponding to the interaction pattern $c$ within the set of all possible actors $\mathcal{A}$. Equivalently, by fixing $\lambda_0=1$, we can rewrite (2): 
\begin{equation}
\lambda^{(c)}_{ij}(t|\boldsymbol{H}^{(c)}_{t-})= \mbox{exp}\Big\{\boldsymbol{\beta}^{(c)T}\boldsymbol{x}^{*(c)}_t(i, j)\Big\}\cdot 1\{j \in \mathcal{A}^{(c)}\}
\end{equation}
where the first element of $\boldsymbol{\beta}^{(c)}$ corresponds to the deviation from $\lambda_0$, by setting $\boldsymbol{x}^{*(c)}_t(i, j)=(\boldsymbol{1}, \boldsymbol{x}^{(c)}_t(i, j))$.\\ \newline
Based on the framework illustrated so far, the likelihood we will use for inference procedure is that of  \cite{PerryWolfe2012}. For each type of interaction pattern $c=1,...,C$, estimation for $\boldsymbol{\beta}^{(c)}$ proceeds by maximizing the so-called partial likelihood of \cite{cox1992regression}: 
\begin{equation}
PL_t(\boldsymbol{\beta}^{(c)})=\prod_{d: c^{(d)}=c} \frac{\mbox{exp}\{\boldsymbol{\beta}^{(c)T}x^{(c)}_{t^{(d)}}(i^{(d)}, j^{(d)})\}}{\sum_{j\in \mathcal{A}^{(c)}} \mbox{exp}\{\boldsymbol{\beta}^{(c)T}x^{(c)}_{t^{(d)}}(i^{(d)}, j)\}},
\end{equation}
where $t^{(d)}$, $i^{(d)}$, and $j^{(d)}$ are the time, sender, and receiver
	of the $d$th document. For computational efficiency, we will use the log-partial likelihood:
\begin{equation}
\mbox{log}PL_t(\boldsymbol{\beta}^{(c)})=\sum_{d: c^{(d)}=c} \Big\{\boldsymbol{\beta}^{(c)T}x^{(c)}_{t^{(d)}}(i^{(d)}, j^{(d)})-\mbox{log}\big[\sum_{j\in \mathcal{A}^{(c)}}\mbox{exp}\{\boldsymbol{\beta}^{(c)T}x^{(c)}_{t^{(d)}}(i^{(d)}, j)\}\big]\Big\}.
\end{equation}
\subsection{Generative Process}
The generative process of this model follows the topic model (LDA) of \cite{Blei2003} and the author-topic model of \cite{rosen2004author}. Same as LDA, documents are represented as random mixtures over latent topics, where each topic is characterized by a distribution over words. However, one crucial difference is that each document is connected to one type of interaction pattern, and the topic distributions vary depending on the assigned interaction pattern. \\ \newline Conditioned on the interaction pattern and their distributions over topics, the process by which a document is generated can be summarized as follows: first, an interaction pattern is chosen by multinomial for each document; next, a topic is sampled for each word from the distribution over topics associated with the interaction pattern of the document; finally, words themselves are sampled from the distribution over words associated with each topic. At the same time, the unique sender-recipient pair of the document is determined by the rate of intensities associated with the interaction pattern and history of interactions until the time the document is written. Below are the detailed generative process for each document in a corpus $D$ and its plate notation (Figure 1), and Table 1 summarizes the notations used in this paper:
\begin{itemize}
	\item[1.] {$\boldsymbol{\phi}^{(k)} \sim \mbox{Dir}(\delta, \bf n)$} \textbf{[See Algorithm 1]}\\
	- A “topic” $k$ is characterized by a discrete distribution over $V$ word types with probability vector $\phi^{(k)}$. A symmetric Dirichlet prior with concentration parameter $\delta$ is placed.
\item[2.] For each of the $C$ interaction patterns \textbf{[See Algorithm 2]}:
\begin{itemize}
	\item[(a)] $\boldsymbol{\beta}^{(c)}\sim \mbox{Normal}(\textbf{0}, \sigma^2I_P)$\\ 
	- The vector of coefficients depends on the interaction pattern $c$. This means that there is variation in the degree of influence from the network statistics.
	\item[(b)] Update $\boldsymbol{x}^{*(c)}_t(i, j)$\\
	- Corpus are partitioned according to the assignment of interaction patterns, and the dynamic network statistics are calculated based on the documents of the same interaction pattern.
	\item[(c)] Using $\boldsymbol{\beta}^{(c)}$ in (a), update $\boldsymbol{\lambda}^{(c)}(t)$\\
	- Use the equation $\lambda^{(c)}_{ij}(t)= \mbox{exp}\Big\{\boldsymbol{\beta}^{(c)T}\boldsymbol{x}^{*(c)}_t(i, j)\Big\}\cdot 1\{j \in \mathcal{A}^{(c)}\}$ for all $i \in \mathcal{A}, j \in \mathcal{A}, i\neq j$.
	\item[(c)] $\boldsymbol{\theta}^{(c)}\sim \mbox{Dir}(\alpha, \textbf{m})$\\
	- Each email has a discrete distribution over topics $\boldsymbol{\theta}^{(c)}$, since the topic proportions for documents in the same cluster are drawn from the same distribution. The Dirichlet parameters $\alpha$ and $\textbf{m}$ may or may not vary by interaction patterns.
\end{itemize}
\item[3.] For each of the $D$ documents \textbf{[See Algorithm 3]}:
\begin{itemize}
	\item[(a)] $c^{(d)}\sim \mbox{Multinomial}(\boldsymbol{\gamma})$\\
	- Each document $d$ is associated with one ``interaction pattern" among $C$ different types, with parameter $\boldsymbol{\gamma}$. Here, we assign the prior for the multinomial parameter $\boldsymbol{\gamma} \sim \mbox{Dir}({\eta}, \boldsymbol{l})$
	\item[(b)] $\mathbf{N}^{(d|c^{(d)})}(t^{(d)}) \sim \mbox{CP}(\boldsymbol{\lambda}^{(c^{(d)})}(t^{(d)}))$\\
	- The actual update of the counting process $\mathbf{N}^{(d|c^{(d)})}(t)$ of the email $d$ is  $N^{(d|c^{(d)})}_{i^{(d)}j^{(d)}}(t^{(d)})=1$ and the rest $N^{(d|c^{(d)})}_{(i, j) \neq (i^{(d)}, j^{(d)})}(t^{(d)})=0$.
\end{itemize}
\item[4.] For each of the $M$ words \textbf{[See Algorithm 4]}:
\begin{itemize}
	\item[(a)] $z_m^{(d)} \sim \mbox{Multinomial}(\boldsymbol{\theta}^{(c^{(d)})})$
\item[(b)] $w_m^{(d)} \sim\mbox{Multinomial} (\phi^{(z_m^{(d)})})$
\end{itemize}
\end{itemize} 
 \begin{algorithm}[H]
 	\SetAlgoLined
 	\caption{Topic Word Distributions}
 	\For{k=1 to K}{
 		draw $\boldsymbol{\phi}^{(k)}$ $\sim$ Dir($\delta, \bf n$)
 	}
 \end{algorithm}
 \begin{algorithm}[H]
 	\SetAlgoLined
 	\caption{Interaction Patterns}
 	\For{c=1 to C}{
 		draw $\boldsymbol{\beta}^{(c)}\sim \mbox{Normal}(\textbf{0}, \sigma^2I_P)$\\
 		set $\boldsymbol{x}^{*(c)}_t(i, j)$ according to Section 2.3\\
 	\For{i=1 to A}{
 	\For{j=1 to A}{
 		\If{i $\neq$ j}{ set $\lambda^{(c)}_{ij}(t)= \mbox{exp}\Big\{\boldsymbol{\beta}^{(c)T}\boldsymbol{x}^{*(c)}_t(i, j)\Big\}\cdot 1\{j \in \mathcal{A}^{(c)}\}$}
 	\Else {set $\lambda^{(c)}_{ij}(t)=0$}
 		}}
 			draw $\boldsymbol{\theta}^{(c)}$ $\sim$ Dir($\alpha, \textbf{m}$)
 	}
 \end{algorithm}
 \begin{algorithm}[H]
 	\SetAlgoLined
 	\caption{Document-Interaction Pattern Assignments}
 	\For{d=1 to D}{
 		draw $c^{(d)}$ $\sim$ Multinomial($\boldsymbol{\gamma}$)\\
 			draw $\mathbf{N}^{(d|c^{(d)})}(t^{(d)}) \sim \mbox{CP}(\boldsymbol{\lambda}^{(c^{(d)})}(t^{(d)}))$
 	}
 \end{algorithm}
 \begin{algorithm}[H]
 	\SetAlgoLined
 	\caption{Tokens}
 	\For{d=1 to $D$}{
 		set ${M}^{(d)}$ = the number of words in document $d$\\
 		\For{m=1 to ${M}^{(d)}$}{
 			draw $z_m^{(d)} \sim \mbox{Multinomial}(\boldsymbol{\theta}^{(c^{(d)})})$\\
 		{draw $w_m^{(d)} \sim\mbox{Multinomial} (\boldsymbol{\phi}^{(z_m^{(d)})})$
 		}}
}
 \end{algorithm}
 \small
 \begin{figure}[ht]
 	\centering
 	\scalebox{0.9}{ \begin{tikzpicture}
 	\tikzstyle{main}=[circle, minimum size = 13mm, thick, draw =black!80, node distance = 10.5mm]
 	\tikzstyle{connect}=[-latex, thick]
 	\tikzstyle{box}=[rectangle, draw=black!100]
 	\node[main, fill = white!100] (gamma) [label=center:$\boldsymbol{\gamma}$] { };
 	\node[main] (c) [below=of gamma,label=center:$c^{(d)}$] { };
 		\node[main, fill = black!10] (N) [right=of c ,label=center:$\mathbf{N}^{(d|c)}(t)$] { };	
 		\node[main] (z) [left=of c,label=center:$z_m^{(d)}$] {};
 	\node[main, fill = black!10] (w) [left=of z,label=center:$w_m^{(d)}$] { };
 	\node[main] (phi) [above=of w,label=center:$\boldsymbol{\phi}^{(k)}$] { };
 	\node[main] (delta) [above=of phi,label=center:$\delta$] { };
 	 			\node[main] (eta) [above=of gamma,label=center:${\eta}$] { };
 	 		 	 						\node[main] (beta) [above=of N ,label=center:$\boldsymbol{\beta}^{(c)}$] { };
 	 		 	 						\node[main, fill = black!10] (x) [right=of beta ,label=center:$\boldsymbol{x}^{(c)}_t{(i,j)}$] { };
 	 		 	 							\node[main] (theta) [above=of z,label=center:$\boldsymbol{\theta}^{(c)}$] { };
 	 		 	 										 	\node[main] (alpha) [above=of theta,label=center:$\alpha$] { };
 	 		 	 						 	 		\node[main] (sigma) [above=of beta,label=center:$\sigma^2$] { };
 	\path (gamma) edge [connect] (c)
 	(z) edge [connect] (w)
 	(theta) edge [connect] (z)
 	(alpha) edge [connect] (theta)
 	(phi) edge [connect] (w)
 	(delta) edge [connect] (phi)
 	(sigma) edge [connect] (beta)
 	(x) edge [connect] (N)
 	(beta) edge [connect] (N)
 	(c) edge [connect] (N)
 	(c) edge [connect] (z)
 		(eta) edge [connect] (gamma);
 			\node[rectangle, inner sep=2.5mm, fit=  (x),label=below left:$A$, xshift=5mm, yshift=5mm] {};
 			\node[rectangle, inner sep=3mm,draw=black!100, fit= (beta)(theta)(x)] {};
 				\node[rectangle, inner sep=4mm, fit=  (beta)(theta)(x) ,label= below left:$C$, xshift=-29mm, yshift=5.5mm] {};
 	\node[rectangle, inner sep=3mm, draw=black!100, fit= (phi)] {};
 		\node[rectangle, inner sep=1.5mm, draw=black!100, fit= (x) ] {};
 		\node[rectangle, inner sep=0mm, fit= (phi),label=below left:$K$, xshift=2.5mm, yshift=1.5mm] {};
 	\node[rectangle, inner sep=0mm, fit= (w),label=below left:$M^{(d)}$, xshift=6.5mm, yshift=2mm] {};
 	\node[rectangle, inner sep=3mm,draw=black!100, fit= (w)(z)] {};
 	\node[rectangle, inner sep=3.5mm, fit= (c) (N) ,label=below left:$D$, xshift=-58mm, yshift=1.5mm] {};
 	\node[rectangle, inner sep=6.2mm, draw=black!100, fit =(c) (z) (w) (N) ] {};
 	\end{tikzpicture}}
 	\caption{Plate notation of IPTM}
 	\label{fig:plate}
 \end{figure}
 \begin{table}[ht]
 	 \centering
\scalebox{0.8}{ 	\begin{tabular}{ |c|c|c|} 
	\hline
 		\hline
 	Authors of the corpus &$\mathcal{A}$ & Set\\
 		\hline
 			Authors of the corpus given interaction pattern $c$ &$\mathcal{A}^{(c)}$ & Set\\
 			\hline
 	Number of authors &$A$ & Scalar \\
 		\hline
 		 	Number of documents &$D$ & Scalar \\
 		 	\hline
 		 	 	Number of words in the $d^{th}$ document &$M^{(d)}$ & Scalar \\
 		 	 	\hline
  	Number of topics & $K$ & Scalar \\
  	\hline
  	  Vocabulary size & $W$ & Scalar \\
  	  	\hline
  	 	Number of interaction patterns &$C$ & Scalar \\
  	 	\hline
  	 		Number of words assigned to interaction pattern and topic&$M^{CK}$ & Scalar \\
  	 		\hline
  	 			Number of words assigned to word and topic&$M^{WK}$ & Scalar \\
  	 			\hline
  	 	Interaction pattern of the $d^{th}$ document&$c^{(d)}$ & Scalar\\
  	 	\hline 
  	 	Time of the $d^{th}$ document&$t^{(d)}$ & Scalar\\
  	 		\hline 
  		Words in the $d^{th}$ document&$\boldsymbol{w}^{(d)}$ & $M^{(d)}$-dimensional vector\\
  		\hline 
  			$m^{th}$ word in the $d^{th}$ document&${w}_m^{(d)}$ & $m^{th}$  component of $\boldsymbol{w}^{(d)}$\\
  			\hline 	
  				Topic assignments in the $d^{th}$ document&$\boldsymbol{z}^{(d)}$ & $M^{(d)}$-dimensional vector\\
  				\hline 
  				Topic assignments for $m^{th}$ word in the $d^{th}$ document&${z}_m^{(d)}$ & $m^{th}$  component of $\boldsymbol{z}^{(d)}$\\
  				\hline 	
  				Dirichlet concentration prior&$\alpha$ & Scalar \\
  					\hline	
  				Dirichlet base prior&$\boldsymbol{m}$ & $K$-dimensional vector \\
  									\hline			
  							Dirichlet concentration prior&$\delta$ & Scalar \\
  							\hline			 
  								Dirichlet base prior&$\boldsymbol{n}$ & $W$-dimensional vector  \\
  								\hline				 	
  									Dirichlet concentration  prior&$\eta$ & Scalar \\
  									\hline		
  										Dirichlet base prior&$\boldsymbol{l}$ & $C$-dimensional vector  \\
  										\hline			
  				Multinomial prior&$\gamma$ & $C$-dimensional vector \\
  				\hline
  				Variance of Normal prior&$\sigma^2$ & Scalar \\
  				\hline		
  					Probabilities of the words given topics &$\Phi$ & $W \times K$ matrix \\
  					\hline		
  						Probabilities of the words given topic $k$ &$\boldsymbol{\phi}^{(k)}$ & $W$-dimensional vector\\
  						\hline
  							Probabilities of the topics given interaction patterns &$\Theta$ & $K \times C$ matrix \\
  							\hline		
  							Probabilities of the topics given interaction pattern $c$ &$\boldsymbol{\theta}^{(c)}$ & $K$-dimensional vector\\
  						\hline		
  						Coefficient of the intensity process given interaction pattern $c$ &$\boldsymbol{\beta}^{(c)}$ & $p$-dimensional vector\\
  							\hline		
  					Network statistics for directed edge $(i, j)$ given interaction pattern $c$ &$\boldsymbol{x}^{(c)}_t{(i,j)}$ & $p$-dimensional vector\\
  						\hline		
  				Counting process in the $d^{th}$ document given interaction pattern &	$\mathbf{N}^{(d|c)}(t)$ & $A\times A$ matrix\\
  						\hline
  						\hline
 	\end{tabular}}
 	\caption {Symbols associated with IPTM, as used in this work}
 	\label{table:SymbolsIPTM}
 \end{table}
\normalsize
\subsection{Dynamic covariates to measure network effects}
The network statistics $\boldsymbol{x}^{(c)}_t(i, j)$ of Equation (2), corresponding to the ordered pair $(i, j)$, can be time-invariant (such as gender) or time-dependent (such as the number of two-paths from $i$ to $j$ just before time $t$). Since time-invariant covariates can be easily specified in various manners (e. g. homophily or group-level effects), here we only consider specification of dynamic covariates.
\begin{figure}[ht]
	\centering
	\includegraphics[width=0.56\textwidth]{PerryWolfe.pdf} 
	\label{fig:PerryWolfeplot}
\end{figure}
\newline Following \cite{PerryWolfe2012} (refer to Fig.3 of \cite{PerryWolfe2012} attached above), we use 6 effects as components of $\boldsymbol{x}^{(c)}_t(i, j)$. The first two behaviors (send and receive) are dyadic, involving exactly two actors,
while the last four (2-send, 2-receive, sibling, and cosibling) are triadic, involving exactly three actors. In addition, we include intercept term and use $\boldsymbol{x}^{*(c)}_t(i, j)$ so that we can estimate the baseline intensities at the same time. However, one different thing from the existing specification is that we define the effects not to be based on finite sub-interval, which require large number of dimention. Instead, we create a single statistic for each effect by incorporating the recency of event into the statistic itself. 
\begin{itemize}[leftmargin=*,rightmargin=-1cm ]
\item [0.] $\mbox{intercept}_t(i, j) = 1$
\item [1.]  $\mbox{send}_t(i, j)=\sum\limits_{d: t^{(d)}<t} I\{i\rightarrow j\}\cdot g(t-t^{(d)})$
\item [2.] $\mbox{receive}_t(i, j)=\sum\limits_{d: t^{(d)}<t} I\{j\rightarrow i\}\cdot g(t-t^{(d)})$
\item [3.] $\mbox{2-send}_t(i, j)=\sum\limits_{h \neq i, j}\Big(\sum\limits_{d: t^{(d)}<t}  I\{i\rightarrow h\}\cdot g(t-t^{(d)})\Big)\Big(\sum\limits_{d: t^{(d)}<t} I\{h\rightarrow j\}\cdot g(t-t^{(d)})\Big)$
\item [4.]  $\mbox{2-receive}_t(i, j)=\sum\limits_{h \neq i, j}\Big(\sum\limits_{d: t^{(d)}<t} I\{h\rightarrow i\}\cdot g(t-t^{(d)})\Big)\Big(\sum\limits_{d: t^{(d)}<t} I\{j\rightarrow h\}\cdot g(t-t^{(d)})\Big)$
\item [5.] $\mbox{sibling}_t(i, j)=\sum\limits_{h \neq i, j}\Big(\sum\limits_{d: t^{(d)}<t} I\{h\rightarrow i\}\cdot g(t-t^{(d)})\Big)\Big(\sum\limits_{d: t^{(d)}<t} I\{h\rightarrow j\}\cdot g(t-t^{(d)})\Big)$
\item [6.] $\mbox{cosibling}_t(i, j)=\sum\limits_{h \neq i, j}\Big(\sum\limits_{d: t^{(d)}<t} I\{i\rightarrow h\}\cdot g(t-t^{(d)})\Big)\Big(\sum\limits_{d: t^{(d)}<t} I\{j\rightarrow h\}\cdot g(t-t^{(d)})\Big)$
\end{itemize}
Here, $g(t-t^{(d)})$ reflects the difference between current time $t$ and the timestamp of previous email $t^{(d)}$, thus measuring the recency. Inspired by the self-exciting Hawkes process, which is often used to model the temporal effect of email data, we can take the exponential kernel $g(t-t^{(d)})=\lambda e^{-\lambda(t-t^{(d)})}$ where $\lambda$ is the parameter of speed at
which sender replies to emails, with larger values indicating faster response times. Indeed, $\lambda^{-1}$ is the expected number of hours it takes to reply to a typical email. For simplicity, in our simulation we fixed $\lambda=0.05$ and multiply 20 in front of exponential function to ensure $g=1$ when $t=t^{(d)}$ (i.e. $g(t-t^{(d)})=20\times 0.05e^{-0.05(t-t^{(d)})}$), but this setup may vary based on the nature of document.
\section{Inference}
The inference for IPTM is similar to that of CPME. In this case, what we actually observe are the tokens $\mathcal{W}=\{\boldsymbol{w}^{(d)} \}_{d=1}^{D}$ and the sender, recipient, and timestamps of the email in the form of the counting process $\mathcal{N}=\{\boldsymbol{N}^{(d)}(t^{(d)}) \}_{d=1}^{D}.$ Next,  $\mathcal{X}=\{\boldsymbol{x}^{(c)}_{t^{(d)}}(i, j)\}_{d=1}^{D}$ is the metadata, and the latent variables are $\Phi=\{\boldsymbol{\phi}^{(k)}\}_{k=1}^{K}, \Theta=\{\boldsymbol{\theta}^{(c)} \}_{c=1}^{C}, \mathcal{Z}=\{\boldsymbol{z}^{(d)} \}_{d=1}^{D}, \mathcal{C}=\{{c}^{(d)} \}_{d=1}^{D},$ and $\mathcal{B}=\{\boldsymbol{\beta}^{(c)} \}_{c=1}^{C}$.\\
\newline 
Below is the the big joint distribution
\begin{equation}
\begin{aligned}
& P(\Phi, \Theta, \mathcal{W}, \mathcal{Z}, \mathcal{C}, \mathcal{B}, \mathcal{N}| \mathcal{X}, \delta, \boldsymbol{n}, \alpha, \boldsymbol{m}, \boldsymbol{\gamma}, \boldsymbol{\eta}, \sigma^2) \\& 
=  P(\mathcal{W}, \mathcal{Z}, \mathcal{C}, \mathcal{B}, \mathcal{N}| \Phi, \Theta, \mathcal{X}, \boldsymbol{\gamma}, \boldsymbol{\eta}, \sigma^2) P(\Phi, \Theta |\delta, \boldsymbol{n}, \alpha, \boldsymbol{m})
\\&= P( \mathcal{W}| \mathcal{Z}, \Phi)P(\mathcal{Z}|\Theta)P(\mathcal{N}|\mathcal{C}, \mathcal{X}, \mathcal{B})P(\mathcal{B}|\mathcal{C}, \sigma^2)P(\Phi|\delta, \boldsymbol{n})P(\Theta|\mathcal{C}, \alpha, \boldsymbol{m})P(\mathcal{C}|\boldsymbol{\gamma})P(\boldsymbol{\gamma}|\boldsymbol{\eta})
\end{aligned}
\end{equation}
Now we can integrate out $\Phi$ and $\Theta$ in latent Dirichlet allocation by applying Dirichlet-multinomial conjugacy as we did in CPME. See APPENDIX A for the detailed steps. After integration, we obtain below:
\begin{equation}
\propto P(\mathcal{W}|\mathcal{Z})P( \mathcal{Z}|\mathcal{C}, \delta, \boldsymbol{n}, \alpha, \boldsymbol{m})P(\mathcal{N}|\mathcal{C}, \mathcal{B}, \mathcal{X})P(\mathcal{B}|\mathcal{C}, \sigma^2)P(\mathcal{C}|\boldsymbol{\gamma})
\end{equation}
Then, we only have to perform inference over the remaining unobserved latent variables $\mathcal{Z}, \mathcal{C},$ and $\mathcal{B}$, using the equation below:
\begin{equation}
P( \mathcal{Z}, \mathcal{C}, \mathcal{B}|\mathcal{W}, \mathcal{N}, \mathcal{X}, \delta, \boldsymbol{n}, \alpha, \boldsymbol{m}, \boldsymbol{\gamma}, \boldsymbol{\eta}, \sigma^2) \propto P(\mathcal{W},  \mathcal{Z}, \mathcal{C}, \mathcal{B}, \mathcal{N} | \mathcal{X}, \delta, \boldsymbol{n}, \alpha, \boldsymbol{m}, \boldsymbol{\gamma}, \boldsymbol{\eta}, \sigma^2)
\end{equation}
 Either Gibbs sampling or Metropolis-Hastings algorithm is applied by sequentially resampling each latent variables from their respective conditional posterior.
  \subsection{Resampling $\mathcal{C}$}
   The first variable we are going to resample is the document-interaction pattern assignments, one document at a time. To obtain the Gibbs sampling equation, which is the posterior conditional probability for the interaction pattern $\mathcal{C}$ for $d^{th}$ document, i.e. $P(c^{(d)}=c|\mathcal{W}, \mathcal{Z},  \mathcal{C}_{\backslash d}, \mathcal{B}, \mathcal{N}, \mathcal{X}, \delta, \boldsymbol{n}, \alpha, \boldsymbol{m}, \boldsymbol{\gamma}, \boldsymbol{\eta}, \sigma^2)$. We can derive the equation as below:
  \begin{equation}
  \begin{aligned} & P(c^{(d)}=c|\mathcal{W}, \mathcal{Z}, \mathcal{C}_{\backslash d}, \mathcal{B}, \mathcal{N}, \mathcal{X}, \delta, \boldsymbol{n}, \alpha, \boldsymbol{m}, \boldsymbol{\gamma}, \boldsymbol{\eta}, \sigma^2)\\
  &\propto P(c^{(d)}=c, \boldsymbol{w}^{(d)}, \boldsymbol{z}^{(d)},  \mathbf{N}^{(d)}{(t^{(d)})}|\mathcal{W}_{\backslash d}, \mathcal{Z}_{\backslash d},\mathcal{C}_{\backslash d}, \mathcal{B}, \mathcal{N}_{\backslash d}, \mathcal{X}, \delta, \boldsymbol{n}, \alpha, \boldsymbol{m}, \boldsymbol{\gamma}, \boldsymbol{\eta}, \sigma^2)\\& \propto P(c^{(d)}=c|\mathcal{C}_{\backslash d}, \boldsymbol{\gamma}) P( \mathbf{N}^{(d)}{(t^{(d)})}| c^{(d)}=c, \mathcal{C}_{\backslash d}, \mathcal{B}, \mathcal{N}_{\backslash d}, \mathcal{X})P(\boldsymbol{w}^{(d)}, \boldsymbol{z}^{(d)}|c^{(d)}=c, \mathcal{W}_{\backslash d}, \mathcal{Z}_{\backslash d}, \mathcal{C}_{\backslash d}, \delta, \boldsymbol{n}, \alpha, \boldsymbol{m}), 
 \end{aligned}
  \end{equation}
 where $P(c^{(d)}=c|\mathcal{C}_{\backslash d}, \boldsymbol{\gamma})$ comes from the multinomial prior $\gamma$ and $P( \mathbf{N}^{(d)}{(t^{(d)})}| c^{(d)}=c, \mathcal{C}_{\backslash d}, \mathcal{B}, \mathcal{N}_{\backslash d}, \mathcal{X})$ is the probability of observing a document with the sender, receiver, and time equal to $(i=i^{(d)}, j=j^{(d)}, t=t^{(d)})$, respectively, given a set of parameter values. We will replace this by the partial likelihood in Equation (4) (without the product term since resampling of $c$ is document-specific). For the last term $P(\boldsymbol{w}^{(d)}, \boldsymbol{z}^{(d)}|c^{(d)}=c, \mathcal{W}_{\backslash d}, \mathcal{Z}_{\backslash d}, \mathcal{C}_{\backslash d}, \delta, \boldsymbol{n}, \alpha, \boldsymbol{m})$, we will follow typical LDA approach. \\ \newline Using Bayes' theorem (See APPENDIX B for conditional probabilty of the last term), we have
   \begin{equation}
   \begin{aligned} &=\Big[ \gamma_{c}\Big]\times\Big[ \frac{\mbox{exp}\{\boldsymbol{\beta}^{(c)T}x^{(c)}_{t^{(d)}}(i^{(d)}, j^{(d)})\}}{\sum_{j\in \mathcal{A}^{(c)}} \mbox{exp}\{\boldsymbol{\beta}^{(c)T}x^{(c)}_{t^{(d)}}(i^{(d)}, j)\}}\Big]\times\Big[\prod_{m=1}^{M^{(d)}}
    \frac{M^{CK}_{cz_m^{(d)}, \backslash d, m}+\alpha m_k}{\sum_{k=1}^KM^{CK}_{ck, \backslash d, m}+\alpha}\Big],
   \end{aligned}
   \end{equation}
where $M^{CK}_{ck}$ is the number of times topic k shows up across the documents of  the interaction pattern $c$. Furthermore, we can take the log of Equation (10) to avoid numerical issue from exponentiation and increase the speed of computation, which becomes:
  	 \begin{equation}
\mbox{log}(\gamma_{c})+\Big(\boldsymbol{\beta}^{(c)T}x^{(c)}_{t^{(d)}}(i^{(d)}, j^{(d)})-\mbox{log}\big[\sum_{j\in \mathcal{A}^{(c)}}\mbox{exp}\{\boldsymbol{\beta}^{(c)T}x^{(c)}_{t^{(d)}}(i^{(d)}, j)\}\big]\Big)+\sum_{m=1}^{M^{(d)}}\mbox{log}(\frac{M^{CK}_{cz_m^{(d)}, \backslash d, m}+\alpha m_k}{\sum_{k=1}^KM^{CK}_{ck, \backslash d, m}+\alpha}).
  	 \end{equation}
  \subsection{Resampling $\mathcal{Z}$}
Next, the new values of $z^{(d)}_m$ are sampled for all of the token topic assignments (one token at a time), using the conditional posterior probability of being topic $k$ as we derived in APPENDIX B:
\begin{equation}
\begin{aligned} & 
 P(z^{(d)}_m=k|\mathcal{W}, \mathcal{Z}_{\backslash d, m},  \mathcal{C}, \mathcal{B}, \mathcal{N}, \mathcal{X}, \delta, \boldsymbol{n}, \alpha, \boldsymbol{m}, \boldsymbol{\gamma}, \boldsymbol{\eta}, \sigma^2)\\
& \propto P(z^{(d)}_m=k, w^{(d)}_m|\mathcal{W}_{\backslash d, m}, \mathcal{Z}_{\backslash d,m}, C, \delta, \boldsymbol{n}, \alpha, \boldsymbol{m})
\end{aligned}
\end{equation}
where the subscript $``{\backslash d, m}"$ denotes the exclsuion of position $m$ in email $d$. In the last line of equation (10), it is the contribution of LDA, so similar to CPME we can write the conditional probability:
	\begin{equation}
	\begin{aligned} 
	& \propto(M^{CK}_{c^{(d)}k, \backslash d, m}+\alpha m_k)\cdot\frac{M_{w_m^{(d)}k, \backslash d, m}^{WK}+\delta n_w}{\sum_{w=1}^WM_{wk,  \backslash d, m}^{WK}+\delta}
	\end{aligned}
	\end{equation}
	which is the well-known form of collapsed Gibbs sampling equation for LDA.
\subsection{Resampling $\mathcal{B}$}
Finally, we wan to update the interaction pattern parameter $\boldsymbol{\beta}^{(c)}$, one interaction pattern at a time. For this, we will use the Metropolis-Hastings algorithm with a proposal density $Q$ being the multivariate Gaussian distribution, with variance $\delta^2_B$ (proposal distirbution variance parameters set by the
user), centered on the current values of $\boldsymbol{\beta}^{(c)}$. Then we draw a proposal $\boldsymbol{\beta}'^{(c)}$ at each iteration. Under symmetric proposal distribution (such as multivariate Gaussian), we cancel out Q-ratio and obtain the acceptance probability equal to:
\begin{equation}
\begin{split}
& \mbox{Acceptance Probability}=
\begin{cases}  \frac{P(\mathcal{B'}|\mathcal{W}, \mathcal{Z}, \mathcal{C}, \mathcal{N}, \mathcal{X})}{P(\mathcal{B}|\mathcal{W}, \mathcal{Z}, \mathcal{C}, \mathcal{N}, \mathcal{X})}\quad\text{if}  <1\\
1 \quad \text{else}
\end{cases}
\end{split}
\end{equation}
After factorization, we get
\begin{equation}
\begin{aligned}
\frac{P(\mathcal{B'}|\mathcal{W},\mathcal{Z}, \mathcal{C}, \mathcal{N}, \mathcal{X})}{P(\mathcal{B}|\mathcal{W}, \mathcal{Z}, \mathcal{C}, \mathcal{N}, \mathcal{X})} &=\frac{P(\mathcal{N}|\mathcal{B'}, \mathcal{W}, \mathcal{Z}, \mathcal{C}, \mathcal{X})P(\mathcal{B'})}{P(\mathcal{N}|\mathcal{B}, \mathcal{W}, \mathcal{Z},  \mathcal{C}, \mathcal{X})P(\mathcal{B})}\\&=\frac{P(\mathcal{N}|\mathcal{C}, \mathcal{X}, \mathcal{B'})P(\mathcal{B'})}{P(\mathcal{N}|\mathcal{C}, \mathcal{X}, \mathcal{B})P(\mathcal{B})},
\end{aligned}
\end{equation}
where $P(\mathcal{N}|\mathcal{C}, \mathcal{X}, \mathcal{B})$ is the partial likelihood in Equation (4).\\ \newline For $P(\mathcal{B})$, we select a multivarate Gaussian priors as mentioned earlier. Similar to what we did in Section 3.1, we can take the log and obtain the log of acceptance ratio as following:
\begin{equation}
\begin{aligned} 
&\mbox{log}\Big(\phi_d(\boldsymbol{\beta}^{\prime(c)};\mathbf{0}, \sigma^2I_P)\Big)-\mbox{log}\Big(\phi_d(\boldsymbol{\beta}^{\prime(c)};\mathbf{0}, \sigma^2I_P)\Big)\\&+\sum_{d:c^{(d)}=c}\Big\{\boldsymbol{\beta}^{\prime(c)T}x^{(c)}_{t^{(d)}}(i^{(d)}, j^{(d)})-\mbox{log}\big[\sum_{j\in \mathcal{A}^{(c)}}\mbox{exp}\{\boldsymbol{\beta}^{\prime(c)T}x^{(c)}_{t^{(d)}}(i^{(d)}, j)\}\big]\Big\}\\&-\sum_{d:c^{(d)}=c} \Big\{\boldsymbol{\beta}^{(c)T}x^{(c)}_{t^{(d)}}(i^{(d)}, j^{(d)})-\mbox{log}\big[\sum_{j\in \mathcal{A}^{(c)}}\mbox{exp}\{\boldsymbol{\beta}^{(c)T}x^{(c)}_{t^{(d)}}(i^{(d)}, j)\}\big]\Big\},
\end{aligned}
\end{equation}
where $\phi_d(\cdot;\mu, \Sigma)$ is the $d$-dimensional multivariate normal density.
Then the log of acceptance ratio we have is:
\begin{equation}
\mbox{log(Acceptance Probability) = min((16), 0) }
\end{equation}
To determine whether we accept the proposed update or not, we take the usual approach, by comparing the log of acceptance ratio we have to the log of a sample from uniform(0,1).\\
\subsection{Pseudocode}
To implement the inference procedure outlined above, we provide a pseudocode for Markov Chain Monte Carlo (MCMC) sampling. Note that we use two loops, outer iteration and inner iteration, in order to avoid the label switching problem \citep{jasra2005markov}, which is an issue caused by the nonidentifiability of the components under symmetric priors in Bayesian mixture modeling. When summarizing model results, we will only use the values from the last $I^{th}$ outer loop because there is no label switching problem within the inner iteration.
 \begin{algorithm}[H]
 	\SetAlgoLined
 	\caption{MCMC($I, n_1, n_2, n_3, \delta_B$ )}
    set initial values $\mathcal{C}^{(0)}, \mathcal{Z}^{(0)},$ and $\mathcal{B}^{(0)}$\\
 	\For{i=1 to I}{
 		\For{n=1 to $n_1$}{
 			fix $\mathcal{Z}=\mathcal{Z}^{(i-1)}$ and $\mathcal{B}=\mathcal{B}^{(i-1)}$ \\
 		  \For{d=1 to D}{
 		  	calculate $\boldsymbol{x}^{*(c)}_{t^{(d)}}(i^{(d)}, j)$ according to Section 2.3, for every $c=1,...,C$\\
	 		 calculate $p^\mathcal{C}|\boldsymbol{z}^{(d)}, \boldsymbol{\beta}^{(c^{(d)})}=(p_1,...,p_C),$ where $p_c=$ exp(Eq. (11) corresponding to $c$)\\
 		  	draw $c^{(d)}\sim \mbox{multinomial}(p^\mathcal{C})$}}
 		\For{n=1 to $n_2$}{
 			fix $\mathcal{C}=\mathcal{C}^{(i)}$ and $\mathcal{B}=\mathcal{B}^{(i-1)}$ \\
 			\For{d=1 to D}{
 		 \For{m=1 to $M^{(d)}$}{
 		 	calculate $p^\mathcal{Z}|\boldsymbol{c}^{(d)}, \boldsymbol{\beta}^{(c^{(d)})}=(p_1,...,p_K),$ where $p_k=$ exp(Eq. (13) corresponding to $k$)\\
 		 	 draw of $z_m^{(d)}\sim\mbox{multinomial}(p^\mathcal{Z})$}}
 		}
 			\For{n=1 to $n_3$}{
 				fix $\mathcal{C}=\mathcal{C}^{(i)}$, $\mathcal{Z}=\mathcal{Z}^{(i)}$, and $\mathcal{B}^{(0)}=$ last value ($n_3^{th}$) of $\mathcal{B}^{(i-1)}$\\
 				calculate $\mathcal{X}=\{\boldsymbol{x}^{*(c)}_{t^{(d)}}(i, j)\}_{d=1}^{D}$ according to Section 2.3, given fixed $\mathcal{C}$\\
 				 \For{c=1 to C}{draw $\boldsymbol{\beta}^{(c)}| \mathcal{C}, \mathcal{Z}, \mathcal{B}^{(n-1)}$ using M-H algorithm in Section 3.3}
 		 }
}	summarize the results using:\\ the last value of $\mathcal{C}$, the last value of $\mathcal{Z}$, and the last $n_3$ length chain of $\mathcal{B}$
 	\end{algorithm}
\section{Application: North Carolina email data}
To see the applicability of the model, we used the North Carolina email data using two counties, Vance county and Dare county, which are the two counties whose email corpus cover the date of Hurricane Sandy (October 22, 2012 – November 2, 2012). Exploratory analysis revealed that Dare county experienced significant change in the pattern of email exchanges; specifically, during the emergency period, email interactions significanty less rely on previous history of interactions, compared to the normal period. On the other hand, Vance county did not experience any distinctive change, and the possible reason for the difference is the locations of two counties. Here we apply IPTM to both data to see the differences in detail, in terms of the interaction patterns and topics of the corpus. One thing to note here is that instead of using 4 different triadic covariates (2-send, 2-receive, sibling, cosibling), we added the four statistics and defined the new statistic `triangles', in order to simply measure triangle effects in general.
\subsection{Vance county email data}
After treating multicast emails (those involving a single sender but multiple receivers) as multiple distinct emails, Vance county data contains 269 emails (only count the email with the number of words greater than 0) between 18 actors, including 620 vocabulary in total. We used $K=20$ topics assuming symmetric Dirichlet prior with the concentration parameter $\alpha=5$, and $C=3$ interaction patterns assuming multinomial prior with parameter $\gamma$ (coming from symmetric Dirichlet prior with the concentration parameter $\eta = 5$). For topic-word distributions, we assumed that $\phi$ follows symmectic Dirichlet distribution with the concentration parameter $\delta=5$. MCMC sampling was implemented based on the order and scheme illustrated in Section 3. We set the outer iteration number as $I=100$, and inner iteration numbers as $n_1=10, n_2=10,$ and $n_3=3500$, which took about 5.5 hours in total. In addition, after some experimentation, $\delta_B$ was set as 0.5, to ensure sufficient acceptance rate (IP1: 0.129, IP2: 0.576, IP3: 0.506). In our case, the average acceptance rate for $\boldsymbol{\beta}$ was 0.260. As demonstrated in Algorithm 5, the last value of $\mathcal{C}$, the last value of $\mathcal{Z}$, and the last $n_3$ length chain of $\mathcal{B}$ were taken as the final posterior samples. Among the $\mathcal{B}$ samples, 500 were discarded as a burn-in, and every 3rd sample was taken for thinning. After these post-processing, MCMC diagnostic plots are attached in APPENDIX C, as well as geweke test statistics. There are some evidence of slightly bad mixing, which could be overcome if we sacrifice computation time and increase the size of thinning or iterations. \\\newline
Below are the summary of IP-topic-word assignments. Each interaction pattern is paired with (a) posterior estimates of dynamic network effects corresponding to the interaction pattern, (b) the top 3 topics most likely to be generated conditioned on the interaction pattern, and (c) the top 10 most likely words to have generated conditioned on the topic and interaction pattern.
\footnotesize
\begin{table}[ht]
	\centering
	\begin{tabular}{|c|c|c|c|} 
		\hline
		& \textbf{IP1} (127 emails) & \textbf{IP2} (56 emails) &\textbf{IP3} (86 emails) \\
		\hline
		\textbf{intercept} &-0.110 [-1.76, 1.84]& -0.189 [-2.04, 1.68]& 0.014 [-1.89, 1.75]\\
		\textbf{send}&  2.345 [1.71, 3.15]& 1.017 [-0.83, 2.86]& 1.021 [-0.21, 2.22]\\
		\textbf{receive}& 1.530 [0.82, 2.31]& 0.770 [-1.26, 2.31]& 1.176 [-0.30, 2.53]\\
		\textbf{triangles} & 0.589 [0.34, 0.82]& 1.418 [-0.04, 2.52]& 0.047 [-1.32, 1.11]\\
			\hline
	\end{tabular}
	\caption {Summary of posterior estimates of $\boldsymbol{\beta}^{(c)}$ for Vance county emails}
	\label{table:Vancebeta}
\end{table}
\newline
\normalsize First, Table 2 summarizes the posterior means and 95\% credible interval for $\boldsymbol{\beta}^{(c)}$ corresponding to each interaction patterns. Below are the several examples of the interpretation of estimates, in the context of point process framework in Section 2.1. The interpretation can be extended to any other interaction patterns and time intervals between the two emails.
\begin{itemize} 
	\item (\textbf{Intercept}) Assuming no history at all between the sender and receiver, the document is $\frac{e^{(0.014)}}{e^{(-0.110)}}\approx 1.132$ times more likely to be IP3 relative to IP1, $\frac{e^{(0.014)}}{e^{(-0.189)}}\approx 1.225$ times more likely to be IP3 relative to IP2, and $\frac{e^{(-0.110)}}{e^{(-0.189)}}\approx 1.082$ times more likely to be IP2 relative to IP1
	\item (\textbf{Send}) If $i$ sends an email to $j$ at time $t$, the likelihoods of $i$ sends email of IP1 to $j$ after 1 hour and 2 hours are multiplied by $e^{(2.345\times e^{(-0.05)})}\approx 9.306$ and $e^{(2.345\times e^{(-2\times 0.05)})}\approx 8.347$, respectively.
\item (\textbf{Receive}) If $j$ sends an email to $i$ at time $t$, the likelihoods of $i$ sends email of IP1 to $j$ after 30 minutes and 30 hours are multiplied by $e^{(1.530\times e^{(-0.5\times 0.05)})}\approx 4.447$ and $e^{(1.530\times e^{(-30\times 0.05)})}\approx 1.407$, respectively.
	\item (\textbf{Triangles}) If $i$ sends/receives an email to/from $k$ at time $t$, and $k$ sends/receives an email to/from $j$ at time $t+1$ (i.e. after 1 hour from $t$), then $i$ sends/receives email to/from $j$ at time $t+2$ (i.e. after 2 hour from $t$) at a slightly higher rate if IP1 (likelihood multiplied by $e^{(0.589\times e^{(-1)}\times e^{(-1)})}\approx 1.083$).
\end{itemize}
By examining the estimates in Table 2 and their corresponding interpretaiton, it seems that there exist strong effects of dynamic network covariates. That is, whether the sender and receiver previously had dyadic or triangle interaction strongly increase the rate of their interactions. Moreover, to see the differences across the interaction patterns more clearly, we compared the posterior distribution using the boxplots in Figure 2 and it seems that there exists notable differences in dynmic network covariates across the interaction patterns. For example, IP1 has the highest send and receive effect, while its baseline intensity (i.e. intercept) or triangle effect is not as high as other interaction patterns. Later, multiple hypothesis testing could be applied in order to test the significance of the differences in  $\boldsymbol{\beta}^{(c)}$ across the $C$ number of interaction patterns.\\
\begin{figure}[ht]
	\centering
	\includegraphics[width=1\textwidth]{boxplot3.pdf} 
	\caption{Posterior distribution of  $\boldsymbol{\beta}^{(c)}$ for Vance county emails}
	\label{fig:Vanceboxplot}
\end{figure}
\newline Next, we scrutinize the topic distributions corresponding to each interaction patterns in Figure 3. There is some distinctive differences in the topic distributions $\mathcal{Z}$, given the assignment of interaction patterns to the documents $\mathcal{C}$. Specifically, each interaction pattern has different topics as the topic with highest probability (IP1: topic 9, IP2: topic 2, IP3: topic 11).
\footnotesize
\begin{figure}[ht]
	\centering
	\includegraphics[width=1\textwidth]{barplot.pdf} 
\caption{Posterior distribution of  $\mathcal{Z}$ for Vance county emails}
\label{fig:Vancebarplot}
\end{figure}
\normalsize
\newline
Furthermore, we look at the distribution of words given the topics, which corresponds to Algorithm 4 in the generative process. Since the topic-word distribution $\phi$ does not depend on the interaction patterns as previous cases, Table 3 lists top 10 topics with top 10 words that have the highest probability conditioned on the topic. In addition, this time we try to check the interaction pattern-word distribution by listing top 10 words that have the highest probability conditioned on the interaction pattern. It seems that the words are not significantly different, having several words like `director', `phones', `department', `description', or `henderson' (county seat of Vance county) appeared repetitively across the most of the topics or interaction patterns. The word 'will' was ranked the top in most of the lists, probably because it was not deleted during the text mining process while other similar type of words like `am', `is', `are', or `can' are all removed. \\
\begin{table}[ht]
	\centering
\scalebox{0.9}{	\begin{tabular}{|l|l||l|l||l|l||l|l||l|l|}
		\hline
		\multicolumn{2}{|l||}{\textbf{Topic 9} (0.0946)}&\multicolumn{2}{l||}{\textbf{Topic 19} (0.0907)}&\multicolumn{2}{l||}{\textbf{Topic 2} (0.0869)}&\multicolumn{2}{l||}{\textbf{Topic 13} (0.0849)}&\multicolumn{2}{l|}{\textbf{Topic 8} (0.0671)}\\
		\hline\hline
		\scriptsize will&\scriptsize 0.0622&\scriptsize will&\scriptsize 0.0487&\scriptsize will&\scriptsize 0.709&\scriptsize will&\scriptsize 0.0237&\scriptsize will&\scriptsize 0.0419\\
		\scriptsize suite&\scriptsize 0.0184&\scriptsize director&\scriptsize  0.0428&\scriptsize director&\scriptsize 0.0370&\scriptsize street&\scriptsize 0.0221&\scriptsize system&\scriptsize 0.0279\\
		\scriptsize henderson&\scriptsize  0.0170&\scriptsize description&\scriptsize 0.0310&\scriptsize department&\scriptsize 0.0277&\scriptsize phones&\scriptsize 0.0189&\scriptsize phones&\scriptsize 0.0259\\
		\scriptsize october&\scriptsize 0.0170&\scriptsize message&\scriptsize 0.0206&\scriptsize henderson&\scriptsize 0.0200&\scriptsize  october&\scriptsize 0.0173&\scriptsize director&\scriptsize 0.0200\\
		\scriptsize system&\scriptsize 0.0170&\scriptsize phones&\scriptsize  0.0177&\scriptsize center&\scriptsize0.0154&\scriptsize fax&\scriptsize 0.0158&\scriptsize center&\scriptsize 0.0180\\
		\scriptsize extension&\scriptsize 0.0156&\scriptsize meeting&\scriptsize 0.0162&\scriptsize phone&\scriptsize 0.0154&\scriptsize suite&\scriptsize 0.0158&\scriptsize  emergency&\scriptsize 0.0160\\
		\scriptsize meeting&\scriptsize 0.0156&\scriptsize fax&\scriptsize  0.0147&\scriptsize october&\scriptsize 0.0139&\scriptsize church&\scriptsize 0.0142&\scriptsize october&\scriptsize 0.0160\\
		\scriptsize phone&\scriptsize 0.0156&\scriptsize latest&\scriptsize  0.0147&\scriptsize street&\scriptsize 0.0139&\scriptsize phone&\scriptsize 0.0142&\scriptsize  attached&\scriptsize 0.0140\\
		\scriptsize electronic&\scriptsize 0.0141&\scriptsize october&\scriptsize  0.0147&\scriptsize church&\scriptsize 0.0123&\scriptsize advised&\scriptsize 0.0139&\scriptsize  department&\scriptsize 0.0140\\
		\scriptsize heads&\scriptsize 0.0141&\scriptsize street&\scriptsize  0.0147&\scriptsize fax&\scriptsize 0.0123&\scriptsize heads&\scriptsize 0.0126&\scriptsize message&\scriptsize0.0140\\
		\hline\hline
	\multicolumn{2}{|l||}{\textbf{Topic 15} (0.0628)}&\multicolumn{2}{l||}{\textbf{Topic 7} ( 0.0566)}&\multicolumn{2}{l||}{\textbf{Topic 3} (0.0435)}&\multicolumn{2}{l||}{\textbf{Topic 6} (0.0427)}&\multicolumn{2}{l|}{\textbf{Topic 16} (0.0400)}\\
	\hline\hline
	\scriptsize will&\scriptsize 0.0554&\scriptsize henderson&\scriptsize 0.0378&\scriptsize heads&\scriptsize 0.0308&\scriptsize department&\scriptsize 0.0502&\scriptsize operations&\scriptsize 0.0268\\
	\scriptsize henderson&\scriptsize 0.0256&\scriptsize will&\scriptsize  0.0355&\scriptsize will&\scriptsize 0.0277&\scriptsize will&\scriptsize 0.0408&\scriptsize phone&\scriptsize 0.0268\\
	\scriptsize phones&\scriptsize 0.0256&\scriptsize phone&\scriptsize 0.0307&\scriptsize director&\scriptsize 0.0246&\scriptsize director&\scriptsize 0.0345&\scriptsize system&\scriptsize 0.0268\\
	\scriptsize phone&\scriptsize 0.0192&\scriptsize church&\scriptsize 0.0189&\scriptsize street&\scriptsize 0.0215&\scriptsize phone&\scriptsize 0.0251&\scriptsize department&\scriptsize 0.0234\\
	\scriptsize street&\scriptsize 0.0192&\scriptsize coming&\scriptsize 0.0189&\scriptsize directory&\scriptsize 0.0185&\scriptsize electronic&\scriptsize 0.0219&\scriptsize church&\scriptsize 0.0201\\
	\scriptsize suite&\scriptsize 0.0192&\scriptsize phones&\scriptsize 0.0189&\scriptsize phone&\scriptsize 0.0185&\scriptsize week&\scriptsize 0.0219&\scriptsize director&\scriptsize 0.0201\\
	\scriptsize cutting&\scriptsize 0.0171&\scriptsize suite&\scriptsize 0.0165&\scriptsize  attached&\scriptsize 0.0154&\scriptsize church&\scriptsize 0.0157&\scriptsize message&\scriptsize 0.0201\\
	\scriptsize department&\scriptsize 0.0171&\scriptsize training&\scriptsize 0.0165&\scriptsize days&\scriptsize 0.0154&\scriptsize description&\scriptsize 0.057&\scriptsize phones&\scriptsize 0.0201\\
	\scriptsize electronic&\scriptsize 0.0149&\scriptsize  attached&\scriptsize 0.0142&\scriptsize description&\scriptsize 0.0154&\scriptsize development&\scriptsize 0.0157&\scriptsize will&\scriptsize 0.0201\\
	\scriptsize rest&\scriptsize 0.0149&\scriptsize description&\scriptsize  0.0142&\scriptsize e-mail&\scriptsize 0.0154&\scriptsize emergency&\scriptsize 0.0157&\scriptsize directory&\scriptsize0.0167\\
	\hline
		\end{tabular}}
	\caption {Summary of top 10 topics with top 10 words that have the highest probability conditioned on the topic}
	\label{table:VancewordsMCMC}
\end{table}
\normalsize
\begin{table}[ht]
	\centering
	\scalebox{0.9}{	\begin{tabular}{|l|l||l|l||l|l|}
			\hline
			\multicolumn{2}{|l||}{\textbf{IP1} (0.4721)}&\multicolumn{2}{l||}{\textbf{IP2} (0.2082)}&\multicolumn{2}{l||}{\textbf{IP3} (0.3197)}\\
			\hline\hline
			\scriptsize will&\scriptsize 0.0505&\scriptsize will&\scriptsize 0.0551&\scriptsize director&\scriptsize 0.257\\
			\scriptsize director&\scriptsize 0.0201&\scriptsize director&\scriptsize  0.0187&\scriptsize will&\scriptsize 0.0216\\
			\scriptsize phones&\scriptsize  0.0198&\scriptsize department&\scriptsize 0.0177&\scriptsize henderson&\scriptsize 0.0199\\
			\scriptsize henderson&\scriptsize 0.0164&\scriptsize description&\scriptsize 0.0167&\scriptsize operations&\scriptsize 0.0193\\
			\scriptsize phone&\scriptsize 0.0164&\scriptsize phones&\scriptsize  0.0152&\scriptsize street&\scriptsize0.0187\\
			\scriptsize department&\scriptsize 0.0159&\scriptsize phone&\scriptsize 0.0141&\scriptsize emergency&\scriptsize 0.0175\\
			\scriptsize street&\scriptsize 0.0156&\scriptsize henderson&\scriptsize  0.0136&\scriptsize fax&\scriptsize 0.0164\\
			\scriptsize system&\scriptsize 0.0148&\scriptsize street&\scriptsize  0.0131&\scriptsize church&\scriptsize 0.0140\\
			\scriptsize october&\scriptsize 0.0132&\scriptsize heads&\scriptsize  0.0126&\scriptsize suite&\scriptsize 0.0140\\
			\scriptsize week&\scriptsize 0.0119&\scriptsize meeting&\scriptsize  0.0111&\scriptsize latest&\scriptsize 0.0134\\
			\hline
		\end{tabular}}
		\caption {Summary of top 10 words that have the highest probability conditioned on the interaction patterns}
		\label{table:VancewordsMCMC2}
	\end{table}
	\normalsize
\newline
Although Vance county email data did not display distinctive idiosyncrasy across the interaction patterns and the topic-token assignments, it is not surprising because Vance county is a small county (land area: 253.52 sq. mi and population: 44,998), and our exploratory data analysis did not find any significant change in the email exchanges of department managers during the period of hurricand Sandy. Yet, it is definitely worthwhile to further look at this in terms of showing the applicability of interaction-partitioned topic model (IPTM), in case of email data. In the next section, we apply the methods for implementing an asymmetric Dirichlet prior in \cite{wallach2008structured} and \cite{wallach2009rethinking}, in hope of improving the model fitting and finding more interesting results in terms of interaction patterns-topics-words relationship.
\newpage
\section{Asymmetric Dirichlet prior over $\Theta$ (topic distribution)}
\cite{wallach2009rethinking} demonstrated that the typical implementations of topic models using symmetric Dirichlet priors with fixed concentration parameters often result in less practical results, which could be our case shown in Section 4.1. Commonly, the model fitting is improved by applying an asymmetric Dirichlet prior over the document–topic distributions (i.e. $\Theta$), while an asymmetric prior over the topic–word distributions (i.e. $\Phi$) provides no real benefit. Therefore, we assign an asymmetric Dirichlet prior over the interaction pattern-topic distributions, $\Theta=\{\boldsymbol{\theta}^{(c)} \}_{c=1}^{C}$, where $\boldsymbol{\theta}^{(c)}$ is drawn from Dir($\alpha, \boldsymbol{m}$). While \cite{wallach2009rethinking} illustrates two different methods, adding a hierarchy to $\Theta$ and optimizing the hyperparameters ($\alpha$ and $\boldsymbol{m}$), we choose to use hyperparameter
optimization steps since it is computationally efficient and also sufficient to achieve the desired performance gains. Now, we assume $\boldsymbol{m}$ to be nonuniform base measures (while $\alpha$ is still a fixed concentration parameter), and implement the hyperparameter optimization technique called ``new fixed-point iterations using the Digamma recurrence relation'' in \cite{wallach2008structured} based on Minka’s fixed-point iteration \citep{minka2000estimating}.
\subsection{New fixed-point iterations using the Digamma recurrence relation}
In this section, we summarize Chapter 2 of \cite{wallach2008structured} and its extension to our IPTM, to illustrate the basic steps and equations used for our optimization. Basically, we want to find the optimal hyperparameter $[\alpha\boldsymbol{m}]^*$ given the data $\mathcal{D}$ such that the probability of the
data given the hyperparameters $P(\mathcal{D}|\alpha\boldsymbol{m})$ is maximized at $[\alpha\boldsymbol{m}]^*$. After incorporating the interaction pattern, the evidence is now given by 
\begin{equation}
P(\mathcal{D}|\alpha\boldsymbol{m})=\prod_{d=1}^{D} \frac{\Gamma(\alpha)}{\Gamma(M^{C\cdot}_{c^{(d)}}+\alpha)}\prod_{k=1}^{K}\frac{\Gamma(M^{CK}_{c^{(d)}k}+\alpha m_k)}{\Gamma(\alpha m_k)}
\end{equation} and is concave in $\alpha \boldsymbol{m}$, thus we will estimate $[\alpha\boldsymbol{m}]^*$ based on some pilot runs of MCMC.\\
\newline First, the starting point is derived by Minka’s fixed-point iteration which takes the derivative of the lower bound $B([\alpha\boldsymbol{m}]^*)$ of $\mbox{log}P(\mathcal{D}|[\alpha\boldsymbol{m}]^*)$ with respect to $[\alpha {m_k}]^*$:
\begin{equation}
[\alpha m_k]^*=\alpha m_k\frac{\sum_{d=1}^{D}\Psi(M^{CK}_{c^{(d)}k}+\alpha m_k)-\Psi(\alpha m_k)}{\sum_{d=1}^{D}\Psi(M^{C\cdot}_{c^{(d)}}+\alpha)-\Psi(\alpha)},
\end{equation}
where $\Psi(\cdot)$ is the first derivative of the log gamma function, known as the digamma function, and the quantity $M^{CK}_{c^{(d)}k}$ is the number of times that outcome $k$ was observed across the documents of the interaction pattern $c^{(d)} \in \{1,...,C\}$. Moreover, the quantity $M^{C\cdot}_{c^{(d)}}=\sum_{k=1}^KM^{CK}_{c^{(d)}k}$ is the total number of words across the documents of the interaction pattern $c^{(d)}\in \{1,...,C\}$. The
value $\alpha m_k$ acts as an initial “pseudocount” for outcome $k$ in all documents.\\ \newline
Next, Wallach's new method rewrites the equation above using the notation $C_k(n)=\sum_{d=1}^{D}\delta(M^{CK}_{c^{(d)}k}-n)$ and $C_\cdot(n)=\sum_{d=1}^{D}\delta(M^{C\cdot}_{c^{(d)}}-n)$:
\begin{equation}
[\alpha m_k]^*=\alpha m_k\frac{\sum_{n=1}^{\mbox{max}_dM^{CK}_{c^{(d)}k}}C_k(n)[\Psi(n+\alpha m_k)-\Psi(\alpha m_k)]}{\sum_{n=1}^{\mbox{max}_dM^{C\cdot}_{c^{(d)}}}C_\cdot(n)[\Psi(n+\alpha)-\Psi(\alpha)]}.
\end{equation}
Finally, applying the digamma recurrence relation (for any positive integer $n$) $$\Psi(n+z)-\Psi(z)=\sum_{f=1}^{n}\frac{1}{f-1+z},$$ we subtitute Equation (20) for below:
\begin{equation}
[\alpha m_k]^*=\alpha m_k\frac{\sum_{n=1}^{\mbox{max}_dM^{CK}_{c^{(d)}k}}C_k(n)\sum_{f=1}^n \frac{1}{f-1+\alpha m_k}}{\sum_{n=1}^{\mbox{max}_dM^{C\cdot}_{c^{(d)}}}C_\cdot(n)\sum_{f=1}^n \frac{1}{f-1+\alpha}}.
\end{equation}
This method is as accurate as Mika's fixed-point iteration method, but it acheives computational efficiency since the digamma recurrence relation reduces the number of new calculations required for each successive $n$ to one. Pseudocode
for the complete fixed-point iteration is given in algorithm 2.2 of \cite{wallach2008structured}.
\begin{comment}
\subsection{Additional layer of hierarchy to $\Theta$}
One alternative to the hyperparameter optimization in Section 5.1. is implementing the additional layer of hierarchy. That is, we assign $\mathbf{m}$ a
Dirichlet prior with a uniform base measure and concentration parameter $\alpha'$, as a symmetric prior for $\boldsymbol{\theta}$ previously used before Section 5. Therefore, the updated plate notation includes the new components $\alpha'$ and $\mathbf{u}$ as below.
\begin{figure}[ht]
	\centering
	\scalebox{0.7}{ \begin{tikzpicture}
		\tikzstyle{main}=[circle, minimum size = 13mm, thick, draw =black!80, node distance = 10.5mm]
		\tikzstyle{connect}=[-latex, thick]
		\tikzstyle{box}=[rectangle, draw=black!100]
		\node[main, fill = white!100] (gamma) [label=center:$\boldsymbol{\gamma}$] { };
		\node[main] (c) [below=of gamma,label=center:$c^{(d)}$] { };
		\node[main, fill = black!10] (N) [right=of c ,label=center:$\mathbf{N}^{(d|c)}(t)$] { };	
		\node[main] (z) [left=of c,label=center:$z_m^{(d)}$] {};
		\node[main, fill = black!10] (w) [left=of z,label=center:$w_m^{(d)}$] { };
		\node[main] (phi) [above=of w,label=center:$\boldsymbol{\phi}^{(k)}$] { };
		\node[main] (delta) [left=of phi,label=center:$\delta$] { };
		\node[main] (eta) [above=of gamma,label=center:${\eta}$] { };
		\node[main] (beta) [above=of N ,label=center:$\boldsymbol{\beta}^{(c)}$] { };
		\node[main, fill = black!10] (x) [right=of beta ,label=center:$\boldsymbol{x}^{(c)}_t{(i,j)}$] { };
		\node[main] (theta) [above=of z,label=center:$\boldsymbol{\theta}^{(c)}$] { };
		\node[main] (m) [above=of theta,label=center:$\mathbf{m} $] { };
		\node[main] (u) [above=of m,label=center:$\mathbf{u} $] { };
		\node[main] (sigma) [above=of beta,label=center:$\sigma^2$] { };
		\node[main] (alphaprime) [left=of u,label=center:$\alpha'$] { };
		\node[main] (alpha) [left=of m,label=center:$\alpha$] { };
		\path (gamma) edge [connect] (c)
		(z) edge [connect] (w)
		(theta) edge [connect] (z)
		(m) edge [connect] (theta)
		(phi) edge [connect] (w)
		(delta) edge [connect] (phi)
		(sigma) edge [connect] (beta)
		(x) edge [connect] (N)
		(beta) edge [connect] (N)
		(c) edge [connect] (N)
		(c) edge [connect] (z)
		(eta) edge [connect] (gamma)
		(alphaprime) edge [connect] (m)
		(alpha) edge [connect] (theta)
		(u) edge [connect] (m);
		\node[rectangle, inner sep=2.5mm, fit=  (x),label=below left:$A$, xshift=5mm, yshift=5mm] {};
		\node[rectangle, inner sep=3mm,draw=black!100, fit= (beta)(theta)(x)] {};
		\node[rectangle, inner sep=4mm, fit=  (beta)(theta)(x) ,label= below left:$C$, xshift=-29mm, yshift=5.5mm] {};
		\node[rectangle, inner sep=3mm, draw=black!100, fit= (phi)] {};
		\node[rectangle, inner sep=1.5mm, draw=black!100, fit= (x) ] {};
		\node[rectangle, inner sep=0mm, fit= (phi),label=below left:$K$, xshift=2.5mm, yshift=1.5mm] {};
		\node[rectangle, inner sep=0mm, fit= (w),label=below left:$M^{(d)}$, xshift=6.5mm, yshift=2mm] {};
		\node[rectangle, inner sep=3mm,draw=black!100, fit= (w)(z)] {};
		\node[rectangle, inner sep=3.5mm, fit= (c) (N) ,label=below left:$D$, xshift=-58mm, yshift=1.5mm] {};
		\node[rectangle, inner sep=6.2mm, draw=black!100, fit =(c) (z) (w) (N) ] {};
		\end{tikzpicture}}
	\caption{Plate notation of updated IPTM}
	\label{fig:plate2}
\end{figure}
\newline\cite{wallach2009rethinking} illustrates that Dirichlet–multinomial conjugacy allows $\mathbf{m}$ to be integrated out so that we do not have to estimate the unknown quantity $\mathbf{m}$. Thus, we modify the resampling equation of $\mathcal{C}$ to reflect the prior for $\mathbf{m}$:
\begin{equation}
\begin{aligned} &P(c^{(d)}=c|\mathcal{W}, \mathcal{Z}, \mathcal{C}_{\backslash d}, \mathcal{B}, \mathcal{N}, \mathcal{X}, \delta, \boldsymbol{n}, \alpha, \alpha' \mathbf{u}, \boldsymbol{\gamma}, \boldsymbol{\eta}, \sigma^2)\\
&\propto\Big[ \gamma_{c}\Big]\times\Big[ \frac{\mbox{exp}\{\boldsymbol{\beta}^{(c)T}x^{(c)}_{t^{(d)}}(i^{(d)}, j^{(d)})\}}{\sum_{j\in \mathcal{A}^{(c)}} \mbox{exp}\{\boldsymbol{\beta}^{(c)T}x^{(c)}_{t^{(d)}}(i^{(d)}, j)\}}\Big]\times\Big[\prod_{m=1}^{M^{(d)}}
\frac{M^{CK}_{cz_m^{(d)}, \backslash d, m}+\alpha \frac{\hat{M}^{K}_{z_m^{(d)}}+\frac{\alpha'}{T}}{\sum_{k=1}^K\hat{M}^{K}_{k}+\alpha'}}{\sum_{k=1}^KM^{CK}_{ck, \backslash d, m}+\alpha}\Big].
\end{aligned}
\end{equation}
Also, we update the resampling equation of $\mathcal{Z}$ in the same manner:
\begin{equation}
\begin{aligned} & 
P(z^{(d)}_m=k|\mathcal{W}, \mathcal{Z}_{\backslash d, m},  \mathcal{C}, \mathcal{B}, \mathcal{N}, \mathcal{X}, \delta, \boldsymbol{n}, \alpha, \alpha' \mathbf{u}, \boldsymbol{\gamma}, \boldsymbol{\eta}, \sigma^2)\\
& \propto(M^{CK}_{c^{(d)}k, \backslash d, m}+\alpha \frac{\hat{M}^{K}_{k}+\frac{\alpha'}{T}}{\sum_{k=1}^K\hat{M}^{K}_{k}+\alpha'})\cdot\frac{M_{w_m^{(d)}k, \backslash d, m}^{WK}+\delta n_w}{\sum_{w=1}^WM_{wk,  \backslash d, m}^{WK}+\delta},
\end{aligned}
\end{equation}
where $\hat{M}^{K}_{k}$ is the
total number of words matched to the topic assignment $k$, until the current update.
\end{comment}
\subsection{Application to Vance county email data}
The optimization procedure in Section 5.1 was applied to Vance county email data, using the settings $K=10$ and $C=3$ but assuming asymmetric base prior $\boldsymbol{m}$ (and not fixing $\alpha$ as well). To see the advantage of optimizing $\alpha$ and $\boldsymbol{m}$ in terms of model fitting, we computed the proportionality constants of $\mbox{log}P(\mathcal{W},\mathcal{Z}|\mathcal{C}, \delta, \boldsymbol{n}, \alpha, \boldsymbol{m})$ in Equation (13) of Section 3.2,  and compared with those of the symmetric model. This time, we fix the parameters as $\alpha=50/K=50/10=5$ with $\boldsymbol{m}=(1,...,1)^T$, $\delta=0.01$, and $\eta=50/C=50/3=16.67$, following the common practice. The asymmetric model uses same $\delta$ and $\eta$, but $\alpha$ and $\boldsymbol{m}$ are optimized for every outer iteration. Figure 4 below compares the MCMC results of symmetric and asymmetric models. 
\begin{figure}[ht]
	\centering
	\includegraphics[width=0.4\textwidth]{logprob.pdf} 
		\includegraphics[width=0.43\textwidth]{logprob2.pdf} 
	\caption{The proportionality constant of $\mbox{log}P(\mathcal{W},\mathcal{Z}|\mathcal{C}, \delta, \boldsymbol{n}, \alpha, \boldsymbol{m})$ for symmetric and asymmetric models, averaged over 5 Gibbs sampling runs (left) and individual runs (right)}
	\label{fig:logPL1}
\end{figure}
\newline Moreover, to measure the within-model consistency of interaction pattern and topic assignments (i.e. similarity between 1) two sets of interaction pattern assignments $\mathcal{C}$ and $\mathcal{C'}$, and 2) two sets of topic
assignments $\mathcal{Z}$ and $\mathcal{Z'}$), we computed the distance between different partitions
of the tokens into $K=10$ topics, using variation
of information (VI) \citep{meilua2003comparing}. Following \cite{wallach2009rethinking}, we calculated
the average VI distance between all 10 unique pairs of topic assignments from the 5 Gibbs runs for the symmetric and asymmetric model, respectively. We also calculated the between-model
VI distance, averaged over all 25 unique pairs of topic assignments for the symmetric and asymmetric model. Since VI measures variablity, the higher the value of VI, the greater the dissimilarity between the two assignments.\\
\footnotesize
\begin{table}[ht]
	\centering
	\begin{tabular}{|c|c|c|} 
		\hline
		\textbf{VI($\mathcal{C}, \mathcal{C'})$}& Symmetric \textbf{$\boldsymbol{m}$} &Asymmetric \textbf{ $\boldsymbol{m}$} \\
		\hline
		Symmetric \textbf{$\boldsymbol{m}$} &  $1.523\pm 0.108$&  $1.590\pm 0.198$\\
		\hline
		Asymmetric \textbf{$\boldsymbol{m}$} & - & $1.717 \pm 0.185$\\
		\hline
	\end{tabular}
	\caption {Average VI distances of $\mathcal{C}$ between 5 runs of symmetric and asymmetric model }
	\label{table:VIforC}
\end{table}
\begin{table}[ht]
	\centering
	\begin{tabular}{|c|c|c|} 
		\hline
		 \textbf{VI($\mathcal{Z}, \mathcal{Z'})$}& Symmetric \textbf{$\boldsymbol{m}$} &Asymmetric \textbf{ $\boldsymbol{m}$} \\
		\hline
		Symmetric \textbf{$\boldsymbol{m}$} &  $3.125\pm 0.212$&  $3.341\pm 0.234$\\
		\hline
		Asymmetric \textbf{$\boldsymbol{m}$} & - & $3.135 \pm 0.160$\\
		\hline
	\end{tabular}
	\caption {Average VI distances of $\mathcal{Z}$ between 5 runs of symmetric and asymmetric model}
	\label{table:VIforZ}
\end{table}
\normalsize
\begin{figure}[ht]
	\centering
	\includegraphics[width=0.5\textwidth]{alpha2.pdf} 
	\includegraphics[width=1.05\textwidth]{histalpha.pdf} 
	\caption{Plots and histograms of concentration parameter values $\alpha$ for 5 Gibbs sampling runs}
	\label{fig:alpha}
\end{figure}
\footnotesize
\begin{figure}[ht]
	\centering
	\includegraphics[width=0.6\textwidth]{topicsym.pdf} 
		\includegraphics[width=0.6\textwidth]{topicasym.pdf} 
	\caption{Posterior distribution of  $\mathcal{Z}$ for Vance county emails, using the Run5}
	\label{fig:topicsymasym}
\end{figure}
\begin{figure}[ht]
	\centering
		\includegraphics[width=0.6\textwidth]{topics.pdf} 
			\caption{Posterior distribution of  $\mathcal{Z}$ for Vance county emails}
			\label{fig:topics}
		\end{figure}
\footnotesize
\begin{table}[ht]
	\centering
	\scalebox{1}{	\begin{tabular}{|l||l||l||l||l|}
			\hline
			{\textbf{Topic 1} (0.2167)}&{\textbf{Topic 8} (0.1285)}&{\textbf{Topic 9} (0.1265)}&{\textbf{Topic 5} (0.1180)}&{\textbf{Topic 2} (0.1096)}\\
			\hline\hline
		church &  directory & suite & will&  henderson\\
		center &  october &  message & phones & street \\
			meeting & jail & request & week& description\\
			heads & provided & cem & phone &  director\\
			office & technology & records &  system &  planning \\
				department & training & attachments & advised & attached\\
			development & cutting & hereto& today & latest\\
			enp & rest &  morning&  latest & email \\
				good & attached & phase & tuesday & wanted \\ 
				pursuant & day &  financial&  monday & excel\\
			\hline
		\end{tabular}}
		\caption {Summary of top 5 topics with top 10 words that have the highest probability conditioned on the topic (Symmetric)}
		\label{table:VancewordsMCMC}
	\end{table}
	\normalsize
\footnotesize
\begin{table}[ht]
	\centering
	\scalebox{1}{	\begin{tabular}{|l||l||l||l||l|}
			\hline
		{\textbf{Topic 2} (0.1787)}&{\textbf{Topic 1} (0.1763)}&{\textbf{Topic 8} (0.1219)}&{\textbf{Topic 9} (0.0996)}&{\textbf{Topic 3} (0.0961)}\\
			\hline\hline
			meesage & fax& will & street &operations \\
			electronic & church & department& emergency&description\\
			time & suite & provided & phone & latest\\
			request & center & cutover & communications & planning\\
			review & office & training & cem & enp\\
			sure & development & cutting & board& e-mail\\
			meeting & good & day & tomorrow & goldvancesealimprovedjordan\\
			heads & email & monday & spread &phase\\
			jail &contacting & questions & sorted &wanted\\
			response & technology &folks & taps &rural\\
				\hline
		\end{tabular}}
		\caption {Summary of top 5 topics with top 10 words that have the highest probability conditioned on the topic (Asymmetric)}
		\label{table:VancewordsMCMC}
	\end{table}
	\normalsize
	\clearpage
\footnotesize
\begin{table}[ht]
	\centering
		\begin{tabular}{|c|c|c|c|} 
			\hline
			\textbf{Symmetric}& \textbf{IP1} (54 emails) & \textbf{IP2} (52 emails) &\textbf{IP3} (163 emails) \\
			\hline
			\textbf{intercept} & 0.341 [-0.66, 1.54]& 1.160 [-0.11, 2.65]& 1.168* [0.32, 2.38]\\
			\textbf{send}&  1.407 [-0.24, 2.98]& 0.297 [-1.66, 2.27]& 1.082* [0.53, 1.75]\\
			\textbf{receive}& -0.621 [-2.56, 1.03]& -0.539 [-2.30, 1.02]& 0.402 [-0.58, 1.22]\\
			\textbf{triangles} & 0.825 [-0.20, 2.05]&  0.472 [-0.60, 1.42]& -0.407 [-1.74, 0.53]\\
			\hline
		\end{tabular}
	\begin{tabular}{|c|c|c|c|} 
		\hline
		\textbf{Asymmetric}& \textbf{IP1} (81 emails) & \textbf{IP2} (26 emails) &\textbf{IP3} (162 emails) \\
		\hline
		\textbf{intercept} &1.316* [0.08, 2.58]& 0.750 [-0.67, 2.34]& 0.646 [-0.267, 1.57]\\
		\textbf{send}&  0.895 [-0.89, 2.64]& 0.170 [-1.41, 1.98]& 1.214* [0.73, 1.64]\\
		\textbf{receive}& -0.242 [-2.18, 1.47]& -0.448 [-2.30, 1.21]& -0.349 [-1.27, 0.41]\\
		\textbf{triangles} & 1.028* [0.29, 1.97]&  1.123 [-0.06, 2.34]& 0731 [0.20, 1.26]\\
		\hline
	\end{tabular}
	\caption {Posterior estimates of $\boldsymbol{\beta}^{(c)}$: Symmetric (upper) and Asymmetric (lower)}
	\label{table:Vancebeta}
\end{table}
\normalsize 	
\begin{figure}[ht]
	\centering
	\includegraphics[width=1.1\textwidth]{betaplots.pdf} 
	\caption{Posterior distribution of  $\boldsymbol{\beta}^{(c)}$ for Vance county emails (Asymmetric)}
	\label{fig:Vanceboxplot}
\end{figure}
\newpage
\normalsize
\subsection{Application to Dare county email data}
After treating multicast emails (those involving a single sender but multiple receivers) as multiple distinct emails, Dare county data contains 4845 emails (only count the email with the number of words greater than 0) between 27 actors, including 2907 vocabulary in total. We used $K=10$ topics assuming both symmetric and asymmetric Dirichlet prior with the concentration parameters $\alpha=50/K=50/10=5$ (symmetric case), and $C=3$ interaction patterns assuming multinomial prior with parameter $\gamma$ (coming from symmetric Dirichlet prior with the concentration parameter $\eta = 50/C=50/3=16.67$). For topic-word distributions, we assumed that $\phi$ follows symmectic Dirichlet distribution with the concentration parameter $\delta=0.01$. MCMC sampling was implemented based on the order and scheme illustrated earlier. We set the outer iteration number as $I=100$, and inner iteration numbers as $n_1=3, n_2=3,$ and $n_3=1500$. In addition, after some experimentation, $\delta_B$ was set as 0.05, to ensure sufficient acceptance rate (IP1: 0.21, IP2: 0.53, IP3: 0.091). In our case, the average acceptance rate for $\boldsymbol{\beta}$ was 0.277. As demonstrated in Algorithm 5, the last value of $\mathcal{C}$, the last value of $\mathcal{Z}$, and the last $n_3$ length chain of $\mathcal{B}$ were taken as the final posterior samples. Among the $\mathcal{B}$ samples, 500 were discarded as a burn-in. After these post-processing, MCMC diagnostic plots are attached in APPENDIX E, as well as geweke test statistics.
\begin{figure}[ht]
	\centering
	\includegraphics[width=0.4\textwidth]{loglikeDare.pdf} 
	\includegraphics[width=0.43\textwidth]{loglikeDare2.pdf} 
	\caption{The proportionality constant of $\mbox{log}P(\mathcal{W},\mathcal{Z}|\mathcal{C}, \delta, \boldsymbol{n}, \alpha, \boldsymbol{m})$ for symmetric and asymmetric models, averaged over 5 Gibbs sampling runs (left) and individual runs (right)}
	\label{fig:logPL1}
\end{figure}
\newline Moreover, to measure the within-model consistency of interaction pattern and topic assignments (i.e. similarity between 1) two sets of interaction pattern assignments $\mathcal{C}$ and $\mathcal{C'}$, and 2) two sets of topic
assignments $\mathcal{Z}$ and $\mathcal{Z'}$), we computed the distance between different partitions
of the tokens into $K=10$ topics, using variation
of information (VI) \citep{meilua2003comparing}. Following \cite{wallach2009rethinking}, we calculated
the average VI distance between all 10 unique pairs of topic assignments from the 5 Gibbs runs for the symmetric and asymmetric model, respectively. We also calculated the between-model
VI distance, averaged over all 25 unique pairs of topic assignments for the symmetric and asymmetric model. 
\footnotesize
\begin{table}[ht]
	\centering
	\begin{tabular}{|c|c|c|} 
		\hline
		\textbf{VI($\mathcal{C}, \mathcal{C'})$}& Symmetric \textbf{$\boldsymbol{m}$} &Asymmetric \textbf{ $\boldsymbol{m}$} \\
		\hline
		Symmetric \textbf{$\boldsymbol{m}$} &  $1.665\pm 0.119$&  $1.709\pm 0.213$\\
		\hline
		Asymmetric \textbf{$\boldsymbol{m}$} & - & $1.594 \pm 0.130$\\
		\hline
	\end{tabular}
	\caption {Average VI distances of $\mathcal{C}$ between 5 runs of symmetric and asymmetric model }
	\label{table:VIforC}
\end{table}
\begin{table}[ht]
	\centering
	\begin{tabular}{|c|c|c|} 
		\hline
		\textbf{VI($\mathcal{Z}, \mathcal{Z'})$}& Symmetric \textbf{$\boldsymbol{m}$} &Asymmetric \textbf{ $\boldsymbol{m}$} \\
		\hline
		Symmetric \textbf{$\boldsymbol{m}$} &  $2.896\pm 0.316$&  $3.309\pm 0.367$\\
		\hline
		Asymmetric \textbf{$\boldsymbol{m}$} & - & $2.904\pm 0.309$\\
		\hline
	\end{tabular}
	\caption {Average VI distances of $\mathcal{Z}$ between 5 runs of symmetric and asymmetric model}
	\label{table:VIforZ}
\end{table}
\normalsize
\begin{figure}[ht]
	\centering
	\includegraphics[width=0.5\textwidth]{alphaplotDare.pdf} 
	\includegraphics[width=1.05\textwidth]{histalphaDare.pdf} 
	\caption{Plots and histograms of concentration parameter values $\alpha$ for 5 Gibbs sampling runs}
	\label{fig:alpha}
\end{figure}
\footnotesize
\begin{figure}[ht]
	\centering
	\includegraphics[width=0.65\textwidth]{Daretopic.pdf} 
	\caption{Posterior distribution of  $\mathcal{Z}$ for Vance county emails, using the Run5}
	\label{fig:topicsymasym}
\end{figure}
\begin{figure}[ht]
	\centering
	\includegraphics[width=0.6\textwidth]{Daretopicoverall.pdf} 
	\caption{Posterior distribution of  $\mathcal{Z}$ for Vance county emails}
	\label{fig:topics}
\end{figure}
\footnotesize
\begin{table}[ht]
	\centering
	\scalebox{1}{	\begin{tabular}{|l||l||l||l||l|}
			\hline
			{\textbf{Topic 4} (0.2106)}&{\textbf{Topic 10} (0.1781)}&{\textbf{Topic 7} (0.1429)}&{\textbf{Topic 3} (0.1223)}&{\textbf{Topic 8} (0.1197)}\\
			\hline\hline
			track &  high &  time & leave&  water\\
			winds &  tides &  monday& hours & hatteras \\
			strom & report& social & media & island\\
			system & position& employees & administrative&  fire\\
			forecast & today & employee &  hour&  winds\\
			munis& white & policy & email& strong\\
			marshall & center & october & resources &schedule\\
			colins & planning& questions &  fax & fyi \\
			manteo & weekend & changes& facebook& power\\ 
			atlantic & issues &  post &  drive & time\\
			\hline
		\end{tabular}}
		\caption {Summary of top 5 topics with top 10 words that have the highest probability conditioned on the topic (Symmetric)}
		\label{table:VancewordsMCMC}
	\end{table}
	\normalsize
	\footnotesize
	\begin{table}[ht]
		\centering
		\scalebox{1}{	\begin{tabular}{|l||l||l||l||l|}
				\hline
				{\textbf{Topic 10} (0.1903)}&{\textbf{Topic 4} (0.1743)}&{\textbf{Topic 8} (0.1454)}&{\textbf{Topic 7} (0.1299)}&{\textbf{Topic 3} (0.1241)}\\
				\hline\hline
				box& munis& storm& hours&administrative\\
				moving & update & law& social&departmnet\\
				report& storm& will & employees& media\\
				outer & status & address & box &employee\\
				shore & track & planning & director &phone\\
				forecast & sandy & email & resources& drive\\
				call & director& day & marshall& leave\\
				long & system & island & collins&hour\\
				moves &coastal & fire & october &policy\\
				board &forecast &week& phr &human\\
				\hline
			\end{tabular}}
			\caption {Summary of top 5 topics with top 10 words that have the highest probability conditioned on the topic (Asymmetric)}
			\label{table:VancewordsMCMC}
		\end{table}
		\normalsize
			\footnotesize
			\begin{table}[ht]
				\centering
				\scalebox{1}{	\begin{tabular}{|l||l||l|}
						\hline
						{\textbf{IP1}}&{\textbf{IP2}}&{\textbf{IP3}}\\
						\hline\hline
						K=7 (0.38), K=3 (0.36) & K=10 (0.32), K=8 (0.22), K=6 (0.18) & K=4 (0.71)\\
						\hline
						time, leave & position, water, update & manteo\\
						monday, hours & issues, island, public & elections\\
						social, media & sheriff,  hatteras, services & storm\\
						employees, administrative & white, strong, board &phone\\
						employee, hour & today, schedule, connection &marshall\\
						policy, email & -lsb-, winds, levels &forecast\\
						october, resources & wanted,  and/or, sandy &drive\\
						questions, facebook & touching, time, current & updates \\ 
						post, fax & report, fire, service & sandy\\
						offices, public & regulations, stats, web &cera\\
						\hline
					\end{tabular}}
					\caption {Summary of top 5 topics with top 10 words that have the highest probability conditioned on the topic (Asymmetric)}
					\label{table:VancewordsMCMC}
				\end{table}
				\normalsize
		\footnotesize
		\begin{table}[ht]
			\centering
			\begin{tabular}{|c|c|c|c|} 
				\hline
				\textbf{Symmetric}& \textbf{IP1} (795 emails) & \textbf{IP2} (2326 emails) &\textbf{IP3} (1724 emails) \\
				\hline
				\textbf{intercept} & 1.763* [1.17, 2.36]& 0.825* [0.61,1.08]& 1.761* [1.60, 2.06]\\
				\textbf{send}&  0.348* [0.06, 0.60]& 0.044 [-0.00, 0.11]& 0.056 [-0.02, 0.15]\\
				\textbf{receive}& 0.129 [-0.20, 0.42]& 0.071 [-0.01, 0.14]& -0.216* [-0.34, -0.10]\\
				\textbf{triangles} & 0.134* [0.06, 0.21]&  0.036 [0.02, 0.04]& 0.009* [0.00, 0.01]\\
				\hline
			\end{tabular}
			\begin{tabular}{|c|c|c|c|} 
				\hline
				\textbf{Asymmetric}& \textbf{IP1} (909 emails) & \textbf{IP2} (1903 emails) &\textbf{IP3} (2033 emails) \\
				\hline
				\textbf{intercept} &0.712* [0.25, 1.09]& 1.285* [1.11, 1.51]& 1.429* [1.29, 1.60]\\
				\textbf{send}&  0.305* [0.04, 0.57]& 0.411* [0.31, 0.53]& 0.040 [-0.03, 0.12]\\
				\textbf{receive}& 0.214 [-0.16, 0.57]& 0.001 [-0.12, 0.091]& -0.242* [-0.40, -0.15]\\
				\textbf{triangles} & -0.056 [-0.16, 0.03]&  0.034 [0.02, 0.04]& 0.014 [0.01, 0.02]\\
				\hline
			\end{tabular}
			\caption {Posterior estimates of $\boldsymbol{\beta}^{(c)}$: Symmetric (upper) and Asymmetric (lower)}
			\label{table:Vancebeta}
		\end{table}
		\normalsize 	
		\begin{figure}[ht]
			\centering
			\includegraphics[width=1.1\textwidth]{betaDareplot.pdf} 
			\caption{Posterior distribution of  $\boldsymbol{\beta}^{(c)}$ for Vance county emails (Asymmetric)}
			\label{fig:Vanceboxplot}
		\end{figure}
\begin{comment}
\section{Asymmetric Dirichlet prior over $\Gamma$ (IP distribution)}
Since the introduction of interaction patterns is the novel aspect of our work, we extend previous Section 5 to our interaction pattern distribution $\Gamma$, in hope of significant improvement in model fitting. 
Therefore, we assign an asymmetric Dirichlet prior over the corpus-interaction pattern distributions, $\Gamma=\{\boldsymbol{\gamma} \}$, where $\boldsymbol{\gamma} $ is drawn from Dir($\eta, \boldsymbol{l}$). Again, we choose to use hyperparameter
optimization steps instead of adding a new hierarchy. Now, we assume $\boldsymbol{l}$ to be nonuniform base measures (while $\eta$ is still a fixed concentration parameter), and implement the hyperparameter optimization technique called ``new fixed-point iterations using the Digamma recurrence relation'' in \cite{wallach2008structured} based on Minka’s fixed-point iteration \citep{minka2000estimating}.
\subsection{New fixed-point iterations using the Digamma recurrence relation}
In this section, we summarize Chapter 2 of \cite{wallach2008structured} to illustrate the basic steps and equations used for our optimization. Basically, we want to find the optimal hyperparameter $[\eta\boldsymbol{l}]^*$ given the data $\mathcal{D}$ such that the probability of the
data given the hyperparameters $P(\mathcal{D}|\eta\boldsymbol{l})$ is maximized at $[\eta\boldsymbol{l}]^*$. The evidence is given by 
\begin{equation}
	P(\mathcal{D}|\eta\boldsymbol{l})=\frac{\Gamma(\eta)}{\Gamma(N_{\cdot}+\eta)}\prod_{c=1}^{C}\frac{\Gamma(N_{c}+\eta{l}_c)}{\Gamma(\eta{l}_c)}
\end{equation} and is concave in $\eta\boldsymbol{l}$, thus we will estimate $[\eta\boldsymbol{l}]^*$ based on some pilot runs of MCMC.\\
\newline First, the starting point is derived by Minka’s fixed-point iteration which takes the derivative of the lower bound $B([\eta\boldsymbol{l}]^*)$ of $\mbox{log}P(\mathcal{D}|[\eta\boldsymbol{l}]^*)$ with respect to $[\eta{l}_c]^*$:
\begin{equation}
	[\eta{l}_c]^*=\eta{l}_c\frac{\Psi(N_{c}+\eta{l}_c)-\Psi(\eta{l}_c)}{\Psi(N_{\cdot}+\eta)-\Psi(\eta)},
\end{equation}
where $\Psi(\cdot)$ is the first derivative of the log gamma function, known as the digamma function, and the quantity $N_{c}$ is the number of times that interaction pattern $c$ was observed in the corpus $D$. Moreover, the quantity $N_{\cdot}=\sum_{c=1}^{C} N_{c}$ is the total number of document in the corpus $D$. The
value $\eta{l}_c$ acts as an initial “pseudocount” for outcome $c$ in all documents.\\ \newline
Applying the digamma recurrence relation (for any positive integer $n$) $$\Psi(n+z)-\Psi(z)=\sum_{f=1}^{n}\frac{1}{f-1+z},$$ we subtitute Equation (20) for below:
\begin{equation}
	[\eta{l}_c]^*=\eta{l}_c\frac{\sum_{f=1}^{N_c} \frac{1}{f-1+\eta{l}_c}}{\sum_{f=1}^{N_\cdot}  \frac{1}{f-1+\eta}}.
\end{equation}
\subsection{Additional layer of hierarchy to $\Theta$}
One alternative to the hyperparameter optimization in Section 5.1. is implementing the additional layer of hierarchy. That is, we assign $\mathbf{m}$ a
Dirichlet prior with a uniform base measure and concentration parameter $\alpha'$, as a symmetric prior for $\boldsymbol{\theta}$ previously used before Section 5. Therefore, the updated plate notation includes the new components $\alpha'$ and $\mathbf{u}$ as below.
\begin{figure}[ht]
\centering
\scalebox{0.7}{ \begin{tikzpicture}
\tikzstyle{main}=[circle, minimum size = 13mm, thick, draw =black!80, node distance = 10.5mm]
\tikzstyle{connect}=[-latex, thick]
\tikzstyle{box}=[rectangle, draw=black!100]
\node[main, fill = white!100] (gamma) [label=center:$\boldsymbol{\gamma}$] { };
\node[main] (c) [below=of gamma,label=center:$c^{(d)}$] { };
\node[main, fill = black!10] (N) [right=of c ,label=center:$\mathbf{N}^{(d|c)}(t)$] { };	
\node[main] (z) [left=of c,label=center:$z_m^{(d)}$] {};
\node[main, fill = black!10] (w) [left=of z,label=center:$w_m^{(d)}$] { };
\node[main] (phi) [above=of w,label=center:$\boldsymbol{\phi}^{(k)}$] { };
\node[main] (delta) [left=of phi,label=center:$\delta$] { };
\node[main] (eta) [above=of gamma,label=center:${\eta}$] { };
\node[main] (beta) [above=of N ,label=center:$\boldsymbol{\beta}^{(c)}$] { };
\node[main, fill = black!10] (x) [right=of beta ,label=center:$\boldsymbol{x}^{(c)}_t{(i,j)}$] { };
\node[main] (theta) [above=of z,label=center:$\boldsymbol{\theta}^{(c)}$] { };
\node[main] (m) [above=of theta,label=center:$\mathbf{m} $] { };
\node[main] (u) [above=of m,label=center:$\mathbf{u} $] { };
\node[main] (sigma) [above=of beta,label=center:$\sigma^2$] { };
\node[main] (alphaprime) [left=of u,label=center:$\alpha'$] { };
\node[main] (alpha) [left=of m,label=center:$\alpha$] { };
\path (gamma) edge [connect] (c)
(z) edge [connect] (w)
(theta) edge [connect] (z)
(m) edge [connect] (theta)
(phi) edge [connect] (w)
(delta) edge [connect] (phi)
(sigma) edge [connect] (beta)
(x) edge [connect] (N)
(beta) edge [connect] (N)
(c) edge [connect] (N)
(c) edge [connect] (z)
(eta) edge [connect] (gamma)
(alphaprime) edge [connect] (m)
(alpha) edge [connect] (theta)
(u) edge [connect] (m);
\node[rectangle, inner sep=2.5mm, fit=  (x),label=below left:$A$, xshift=5mm, yshift=5mm] {};
\node[rectangle, inner sep=3mm,draw=black!100, fit= (beta)(theta)(x)] {};
\node[rectangle, inner sep=4mm, fit=  (beta)(theta)(x) ,label= below left:$C$, xshift=-29mm, yshift=5.5mm] {};
\node[rectangle, inner sep=3mm, draw=black!100, fit= (phi)] {};
\node[rectangle, inner sep=1.5mm, draw=black!100, fit= (x) ] {};
\node[rectangle, inner sep=0mm, fit= (phi),label=below left:$K$, xshift=2.5mm, yshift=1.5mm] {};
\node[rectangle, inner sep=0mm, fit= (w),label=below left:$M^{(d)}$, xshift=6.5mm, yshift=2mm] {};
\node[rectangle, inner sep=3mm,draw=black!100, fit= (w)(z)] {};
\node[rectangle, inner sep=3.5mm, fit= (c) (N) ,label=below left:$D$, xshift=-58mm, yshift=1.5mm] {};
\node[rectangle, inner sep=6.2mm, draw=black!100, fit =(c) (z) (w) (N) ] {};
\end{tikzpicture}}
\caption{Plate notation of updated IPTM}
\label{fig:plate2}
\end{figure}
\newline\cite{wallach2009rethinking} illustrates that Dirichlet–multinomial conjugacy allows $\mathbf{m}$ to be integrated out so that we do not have to estimate the unknown quantity $\mathbf{m}$. Thus, we modify the resampling equation of $\mathcal{C}$ to reflect the prior for $\mathbf{m}$:
\begin{equation}
\begin{aligned} &P(c^{(d)}=c|\mathcal{W}, \mathcal{Z}, \mathcal{C}_{\backslash d}, \mathcal{B}, \mathcal{N}, \mathcal{X}, \delta, \boldsymbol{n}, \alpha, \alpha' \mathbf{u}, \boldsymbol{\gamma}, \boldsymbol{\eta}, \sigma^2)\\
&\propto\Big[ \gamma_{c}\Big]\times\Big[ \frac{\mbox{exp}\{\boldsymbol{\beta}^{(c)T}x^{(c)}_{t^{(d)}}(i^{(d)}, j^{(d)})\}}{\sum_{j\in \mathcal{A}^{(c)}} \mbox{exp}\{\boldsymbol{\beta}^{(c)T}x^{(c)}_{t^{(d)}}(i^{(d)}, j)\}}\Big]\times\Big[\prod_{m=1}^{M^{(d)}}
\frac{M^{CK}_{cz_m^{(d)}, \backslash d, m}+\alpha \frac{\hat{M}^{K}_{z_m^{(d)}}+\frac{\alpha'}{T}}{\sum_{k=1}^K\hat{M}^{K}_{k}+\alpha'}}{\sum_{k=1}^KM^{CK}_{ck, \backslash d, m}+\alpha}\Big].
\end{aligned}
\end{equation}
Also, we update the resampling equation of $\mathcal{Z}$ in the same manner:
\begin{equation}
\begin{aligned} & 
P(z^{(d)}_m=k|\mathcal{W}, \mathcal{Z}_{\backslash d, m},  \mathcal{C}, \mathcal{B}, \mathcal{N}, \mathcal{X}, \delta, \boldsymbol{n}, \alpha, \alpha' \mathbf{u}, \boldsymbol{\gamma}, \boldsymbol{\eta}, \sigma^2)\\
& \propto(M^{CK}_{c^{(d)}k, \backslash d, m}+\alpha \frac{\hat{M}^{K}_{k}+\frac{\alpha'}{T}}{\sum_{k=1}^K\hat{M}^{K}_{k}+\alpha'})\cdot\frac{M_{w_m^{(d)}k, \backslash d, m}^{WK}+\delta n_w}{\sum_{w=1}^WM_{wk,  \backslash d, m}^{WK}+\delta},
\end{aligned}
\end{equation}
where $\hat{M}^{K}_{k}$ is the
total number of words matched to the topic assignment $k$, until the current update.
\end{comment}

		\clearpage
\section*{APPENDIX}
\subsection*{APPENDIX A: Deriving the sampling equations for IPTM}
\begin{equation}
\begin{aligned}
& P(\Phi, \Theta, \mathcal{W}, \mathcal{Z}, \mathcal{C}, \mathcal{B}, \mathcal{N}| \mathcal{X}, \delta, \boldsymbol{n}, \alpha, \boldsymbol{m}, \boldsymbol{\gamma}, \boldsymbol{\eta}, \sigma^2) \\& 
=  P(\mathcal{W}, \mathcal{Z}, \mathcal{C}, \mathcal{B}, \mathcal{N}| \Phi, \Theta, \mathcal{X}, \boldsymbol{\gamma}, \boldsymbol{\eta}, \sigma^2) P(\Phi, \Theta |\delta, \boldsymbol{n}, \alpha, \boldsymbol{m})
\\&= P( \mathcal{W}| \mathcal{Z}, \Phi)P(\mathcal{Z}|\Theta)P(\mathcal{N}|\mathcal{C}, \mathcal{B}, \mathcal{X})P(\mathcal{B}|\mathcal{C}, \sigma^2)P(\Phi|\delta, \boldsymbol{n})P(\Theta|\mathcal{C}, \alpha, \boldsymbol{m})P(\mathcal{C}|\boldsymbol{\gamma})P(\boldsymbol{\gamma}|\boldsymbol{\eta})
\\&= \Big[\prod_{d=1}^{D}\prod_{m=1}^{M^{(d)}} P(w_m^{(d)}| \phi_{z_m^{(d)}})\Big]\times \Big[\prod_{d=1}^{D}\prod_{m=1}^{M^{(d)}} P( z_m^{(d)}| \boldsymbol{\theta}^{(c)})\Big]\times \Big[\prod_{d=1}^{D} P( \mathbf{N}^{(d)}(t^{(d)})|c^{(d)}, \boldsymbol{x}^{(c^{(d)})}(t^{(d)}), \boldsymbol{\beta}^{(c)})\Big]  \\&\quad \quad \times\Big[\prod_{c=1}^{C} P( \boldsymbol{\beta}^{(c)}| \sigma^2)\Big]\times\Big[\prod_{k=1}^{K} P( \boldsymbol{\phi}^{(k)}| \delta, \boldsymbol{n})\Big]\times \Big[\prod_{c=1}^{C} P( \boldsymbol{\theta}^{(c)}|\alpha, \boldsymbol{m})\Big]\times \Big[\prod_{d=1}^{D} P(c^{(d)}|\boldsymbol{\gamma})\Big]  \times P(\boldsymbol{\gamma}|\boldsymbol{\eta})
\end{aligned}
\end{equation}
Since $P(\boldsymbol{\beta}^{(c)}| \sigma^2)$ is $\mbox{Normal}(\boldsymbol{0}, \sigma^2)$ and $P(\boldsymbol{\gamma}|\boldsymbol{\eta})$ is $\mbox{Dirichlet}(\boldsymbol{\eta})$, we can drop the two terms out and further rewrite the equation (20) as below:
\begin{equation}
\begin{aligned}
& \propto \Big[\prod_{d=1}^{D}\prod_{m=1}^{M^{(d)}} P(w_m^{(d)}| \phi_{z_m^{(d)}})\Big]\times \Big[\prod_{d=1}^{D}\prod_{m=1}^{M^{(d)}} P(z_m^{(d)}| \boldsymbol{\theta}^{(c)})\Big]\times \Big[\prod_{d=1}^{D} P( \mathbf{N}^{(d)}(t^{(d)})|c^{(d)}, \boldsymbol{x}^{(c^{(d)})}(t^{(d)}), \boldsymbol{\beta}^{(c)})\Big]\\& \quad \quad  \times\Big[\prod_{k=1}^{K} P( \boldsymbol{\phi}^{(k)}| \delta, \boldsymbol{n})\Big] \times\Big[\prod_{c=1}^{C} P( \boldsymbol{\theta}^{(c)}|\alpha, \boldsymbol{m})\Big] \times\Big[\prod_{d=1}^{D} P(c^{(d)}|\boldsymbol{\gamma})\Big] \\&
= \Big[\prod_{d=1}^{D}\prod_{m=1}^{M^{(d)}} \phi_{w_m^{(d)}z_m^{(d)}}\Big]\times \Big[\prod_{d=1}^{D}\prod_{m=1}^{M^{(d)}} \boldsymbol{\theta}^{(c)}_{z_m^{(d)}}\Big]\times\Big[\prod_{d=1}^{D} \frac{\mbox{exp}\{\boldsymbol{\beta}^{(c)T}x^{(c^{(d)})}_{t^{(d)}}(i^{(d)}, j^{(d)})\}}{\sum_{j\in \mathcal{A}^{(c)}} \mbox{exp}\{\boldsymbol{\beta}^{(c)T}x^{(c^{(d)})}_{t^{(d)}}(i^{(d)}, j)\}}\Big]\\& \quad \quad \times \Big[\prod_{k=1}^{K} \Big(\frac{\Gamma(\sum_{w=1}^{W}\delta n_w)}{\prod_{w=1}^{W}\Gamma(\delta n_w)}\prod_{w=1}^{W}\phi_{wk}^{\delta n_w-1} \Big)\Big]\times \Big[\prod_{c=1}^{C} \Big(\frac{\Gamma(\sum_{k=1}^{K}\alpha m_k)}{\prod_{k=1}^{K}\Gamma(\alpha m_k)}\prod_{k=1}^{K}(\boldsymbol{\theta}^{(c)}_{k})^{\alpha m_k-1} \Big)\Big] \times\Big[\prod_{d=1}^{D} \gamma_{c}^{I(c^{(d)}=c)}\Big] \\&
=\Big[\frac{\Gamma(\sum_{w=1}^{W}\delta n_w)}{\prod_{w=1}^{W}\Gamma(\delta n_w)}\Big]^K \times \Big[\frac{\Gamma(\sum_{w=1}^{W}\delta n_w)}{\prod_{w=1}^{W}\Gamma(\delta n_w)}\Big]^C \times\Big[\prod_{d=1}^{D} \frac{\mbox{exp}\{\boldsymbol{\beta}^{(c)T}x^{(c^{(d)})}_{t^{(d)}}(i^{(d)}, j^{(d)})\}}{\sum_{j\in \mathcal{A}^{(c)}} \mbox{exp}\{\boldsymbol{\beta}^{(c)T}x^{(c^{(d)})}_{t^{(d)}}(i^{(d)}, j)\}}\Big]\\&\quad\quad\times\Big[\prod_{d=1}^{D}\gamma_{c^{(d)}}\Big]\times
\Big[\prod_{k=1}^{K}\prod_{w=1}^{W}\phi_{wk}^{M^{WK}_{wk}+\delta n_w-1}\Big]\times\Big[\prod_{c=1}^{C}\prod_{k=1}^{K}(\boldsymbol{\theta}^{(c)}_{k})^{M^{CK}_{ck}+\alpha m_k-1}\Big]
\end{aligned}
\end{equation}
where $M^{WK}_{wk}$ is the number of times the $w^{th}$ word in the vocabulary is assigned to topic $k$, and $M^{CK}_{ck}$ is the number of times topic k shows up given the interaction pattern $c$. By looking at the forms of the terms involving  $\Theta$ and $\Phi$ in Equation (21), we integrate out the random variables $\Theta$ and $\Phi$, making use of the fact that the Dirichlet distribution is a conjugate prior of multinomial distribution. Applying the well-known formula $\int\prod_{m=1}^{M}[x_m^{k_m-1}dx_m]=\frac{\prod_{m=1}^M\Gamma(k_m)}{\Gamma(\sum_{m=1}^Mk_m)}$ to (22), we have:
\begin{equation}
\begin{aligned}
&P(\mathcal{W}, \mathcal{Z}, \mathcal{C}, \mathcal{B}, \mathcal{N}| \mathcal{X}, \delta, \boldsymbol{n}, \alpha, \boldsymbol{m}, \boldsymbol{\gamma}, \boldsymbol{\eta}, \sigma^2)\\&=\mbox{Const.}\int_{\Theta}\int_{\Phi}\Big[\prod_{k=1}^{K}\prod_{w=1}^{W}\phi_{wk}^{M^{WK}_{wk}+\delta n_w-1}\Big]\Big[\prod_{c=1}^{C}\prod_{k=1}^{K}(\boldsymbol{\theta}^{(c)}_{k})^{M^{CK}_{ck}+\alpha m_k-1}\Big]d\Phi d\Theta
\\&=\mbox{Const.}\Big[\prod_{k=1}^{K}\int_{\phi_{:k}}\prod_{w=1}^{W}\phi_{wk}^{M^{WK}_{wk}+\delta n_w-1  }d\phi_{:k}\Big]\times\Big[\prod_{c=1}^{C}\int_{\theta_{:c}}\prod_{k=1}^{K}(\boldsymbol{\theta}^{(c)}_{k})^{M^{CK}_{ck}+\alpha m_k-1}d\theta_{:c}\Big]
\\&=\mbox{Const.}\Big[\prod_{k=1}^{K}\frac{\prod_{w=1}^W\Gamma(M_{wk}^{WK}+\delta n_w)}{\Gamma(\sum_{w=1}^WM_{wk}^{WK}+\delta )}\Big]\times\Big[\prod_{c=1}^{C}\frac{\prod_{k=1}^K\Gamma(M^{CK}_{ck}+\alpha m_k)}{\Gamma(\sum_{k=1}^KM^{CK}_{ck}+\alpha)}\Big].
\end{aligned}
\end{equation}
\subsection*{APPENDIX B: Computing conditional probability}
\begin{equation}
\begin{aligned}
& P(\boldsymbol{w}^{(d)}, \boldsymbol{z}^{(d)}|c^{(d)}=c, \mathcal{W}_{\backslash d}, \mathcal{Z}_{\backslash d}, \mathcal{C}_{\backslash d}, \delta, \boldsymbol{n}, \alpha, \boldsymbol{m}) \\& \propto \prod_{m=1}^{M^{(d)}}P(z^{(d)}_m=k, w^{(d)}_m=w| c^{(d)}=c, \mathcal{W}_{\backslash d, m}, \mathcal{Z}_{\backslash d,m}, \mathcal{C}_{\backslash d}, \delta, \boldsymbol{n}, \alpha, \boldsymbol{m})
\end{aligned}
\end{equation} 
To obtain the Gibbs sampling equation, we need to obtain an expression for $P(z^{(d)}_m=k,  w^{(d)}_m=w, c^{(d)}=c|\mathcal{W}_{\backslash d}, \mathcal{Z}_{\backslash d}, \mathcal{C}_{\backslash d}, \delta, \boldsymbol{n}, \alpha, \boldsymbol{m})$,
 From Bayes' theorem and Gamma identity $\Gamma(k+1)=k\Gamma(k)$,
\begin{equation}
\begin{aligned}
& P(z^{(d)}_m=k, w^{(d)}_m=w, c^{(d)}=c|\mathcal{W}_{\backslash d, m}, \mathcal{Z}_{\backslash d,m}, \mathcal{C}_{\backslash d}, \delta, \boldsymbol{n}, \alpha, \boldsymbol{m}) \\& \propto 
\frac{P(\mathcal{W}, \mathcal{Z}, \mathcal{C}|\delta, \boldsymbol{n}, \alpha, \boldsymbol{m})}{P(\mathcal{W}_{\backslash d, m}, \mathcal{Z}_{\backslash d, m}, \mathcal{C}|\delta, \boldsymbol{n}, \alpha, \boldsymbol{m})}\\& \propto \frac{\prod_{k=1}^{K}\frac{\prod_{w=1}^W\Gamma(M_{wk}^{WK}+\delta n_w)}{\Gamma(\sum_{w=1}^WM_{wk}^{WK}+\delta )}\times\prod_{c=1}^{C}\frac{\prod_{k=1}^K\Gamma(M^{CK}_{ck}+\alpha m_k)}{\Gamma(\sum_{k=1}^KM^{CK}_{ck}+\alpha)}}{\prod_{k=1}^{K}\frac{\prod_{w=1}^W\Gamma(M_{wk, \backslash d, m}^{WK}+\delta n_w)}{\Gamma(\sum_{w=1}^WM_{wk, \backslash d, m}^{WK}+\delta )}\times\prod_{c=1}^{C}\frac{\prod_{k=1}^K\Gamma(M^{CK}_{ck, \backslash d, m}+\alpha m_k)}{\Gamma(\sum_{k=1}^KM^{CK}_{ck, \backslash d, m}+\alpha)}}\\ & \propto 
\frac{M_{wk, \backslash d, m}^{WK}+\delta n_w}{\sum_{w=1}^WM_{wk,  \backslash d, m}^{WK}+\delta}\times\frac{M^{CK}_{ck, \backslash d, m}+\alpha m_k}{\sum_{k=1}^KM^{CK}_{ck, \backslash d, m}+\alpha}
\end{aligned}
\end{equation}
Then, the conditional probability that a novel word generated in the document of interaction pattern $c^{(d)}=c$  would be assigned to topic $z_m^{(d)}=k$ is obtained by:
 \begin{equation}
 \begin{aligned}
 &P(z^{(d)}_m=k|w^{(d)}_m=w, c^{(d)}=c, \mathcal{W}_{\backslash d, m}, \mathcal{Z}_{\backslash d,m}, \mathcal{C}_{\backslash d}, \delta, \boldsymbol{n}, \alpha, \boldsymbol{m}) \\& \propto
 \frac{M^{CK}_{ck, \backslash d, m}+\alpha m_k}{\sum_{k=1}^KM^{CK}_{ck, \backslash d, m}+\alpha}
 \end{aligned}
 \end{equation}
In addition, the conditional probability that a new word generated in the document would be $w_m^{(d)}=w$, given that it is generated from topic $z_m^{(d)}=k$ is obtained by:
\begin{equation}
\begin{aligned}
& P(w^{(d)}_m=w|z^{(d)}_m=k, c^{(d)}=c, \mathcal{W}_{\backslash d, m}, \mathcal{Z}_{\backslash d,m}, \mathcal{C}_{\backslash d}, \delta, \boldsymbol{n}, \alpha, \boldsymbol{m}) \\& \propto 
\frac{M_{wk, \backslash d, m}^{WK}+\delta n_w}{\sum_{w=1}^WM_{wk, \backslash d, m}^{WK}+\delta}
\end{aligned} 
 \end{equation}
 \subsection*{APPENDIX C: MCMC Diagnostics for Vance county emails}
 \begin{figure}[ht]
 	\centering
 	\includegraphics[width=0.7\textwidth]{Entropy.pdf} 
 	\caption{Plot of empirical entropy to check the distribution of IP assignments $\mathcal{C}$ }
 	\label{fig:IP1}
 \end{figure}
  \begin{figure}[ht]
 	\centering
 	\includegraphics[width=0.7\textwidth]{IP1diag.pdf} 
 		\caption{Traceplots and density plots of $\boldsymbol{\beta}^{(1)}$}
 	\label{fig:IP1}
 	 \end{figure}
\footnotesize
\begin{verbatim}
> geweke.diag(mcmc)

Fraction in 1st window = 0.1
Fraction in 2nd window = 0.5 

var1    var2    var3    var4 
-3.7668  0.0945  0.4176 -0.2620 
\end{verbatim} 	
\begin{figure}[ht]
	\centering
	\includegraphics[width=0.7\textwidth]{IP2diag.pdf} 
	\caption{Traceplots and density plots of $\boldsymbol{\beta}^{(2)}$}
	\label{fig:IP2}
\end{figure}
\footnotesize
\begin{verbatim}
> geweke.diag(mcmc)

Fraction in 1st window = 0.1
Fraction in 2nd window = 0.5 

var1     var2     var3     var4 
0.02362 -0.22385  0.61747 -0.05824 
\end{verbatim} 	 \clearpage
\begin{figure}[ht]
	\centering
	\includegraphics[width=0.7\textwidth]{IP3diag.pdf} 
	\caption{Traceplots and density plots of $\boldsymbol{\beta}^{(3)}$}
	\label{fig:IP3}
\end{figure}
\footnotesize
\begin{verbatim}
> geweke.diag(mcmc)

Fraction in 1st window = 0.1
Fraction in 2nd window = 0.5 

var1    var2    var3    var4 
0.5025  0.5471 -0.7871 -0.2434 
\end{verbatim} 	 
 \subsection*{APPENDIX D: MCMC Diagnostics for Vance county emails - updated}
 \begin{figure}[ht]
 	\centering
 	\includegraphics[width=0.54\textwidth]{IP1symdiag.pdf} 
 	 	\includegraphics[width=0.45\textwidth]{IP1asymdiag.pdf} 
 	 		\includegraphics[width=0.45\textwidth]{IP2symdiag.pdf} 
 	 		\includegraphics[width=0.54\textwidth]{IP2asymdiag.pdf} 
 	 			\includegraphics[width=0.45\textwidth]{IP3symdiag.pdf} 
 	 			\includegraphics[width=0.54\textwidth]{IP3asymdiag.pdf} 
 	\caption{Traceplots and density plots of $\boldsymbol{\beta}$: Symmetric (left) and Asymmetric (right)}
 	\label{fig:IP1sym}
 \end{figure}
\bibliographystyle{apalike}
\bibliography{BominBib}

\end{document}