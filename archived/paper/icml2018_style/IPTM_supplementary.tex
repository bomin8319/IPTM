%%%%%%%% ICML 2018 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{amsmath}
\usepackage{booktabs} % for professional tables
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2018} with \usepackage[nohyperref]{icml2018} above.
%\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage{icml2018}

% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{icml2018}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{A Network Model for Dynamic Textual Communications with Application to Government Email Corpora}

\begin{document}

\twocolumn[
\icmltitle{Suplementary Materials for ``A Network Model for Dynamic Textual Communications with Application to Government Email Corpora"}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2018
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
%\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Bomin Kim}{to}
\icmlauthor{Aaron Schein}{goo}
\icmlauthor{Bruce Desmarais}{ed}
\icmlauthor{Hanna Wallach}{equal,to}
\end{icmlauthorlist}

\icmlaffiliation{to}{Department of Statistics, Pennsylvania State University, Pennsylvania, USA}
\icmlaffiliation{goo}{College of Information and Computer Sciences, University of Massachusetts Amherst, Massachusetts, USA}
\icmlaffiliation{ed}{Department of Political Science, Pennsylvania State University,Pennsylvania, USA}
\icmlaffiliation{equal}{Microsoft Research NYC, New York, USA}

\icmlcorrespondingauthor{Cieua Vvvvv}{c.vvvvv@googol.com}
\icmlcorrespondingauthor{Eee Pppp}{ep@eden.co.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
%\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\section{Normalizing constant of Gibbs measure}\label{sec: non-empty Gibbs measure}
 	 The non-empty Gibbs measure \cite{fellows2017removing} defines the probability of author $a$ selecting the binary recipient vector $\boldsymbol{u}_{ad}$ as
 	 \begin{equation*} 
 	 	\begin{aligned}
 	 		& P(\boldsymbol{u}_{ad}| \delta, \boldsymbol{\lambda}_{ad} ) \\&= \frac{\exp\Big\{ \mbox{log}\big(\text{I}(\lVert \boldsymbol{u}_{ad} \rVert_1 > 0)\big) + \sum_{r \neq a} (\delta+\lambda_{adr})u_{adr} \Big\}}{Z(\delta,\boldsymbol{\lambda}_{ad})}.
 	 	\end{aligned}
 	 \end{equation*}
 	 
 	 To use this distribution efficiently, we derive a closed-form expression for $Z(\delta,\boldsymbol{\lambda}_{id})$ that does not require brute-force summation over the support of $\boldsymbol{u}_{ad}$ (\textit{i.e.} $\forall \boldsymbol{u}_{ad} \in [0,1]^A$). We recognize that if $\boldsymbol{u}_{ad}$ were drawn via independent Bernoulli distributions in which $P({u}_{adr}=1|\delta, \boldsymbol{\lambda}_{ad})$ was given by logit$(\delta+\lambda_{adr})$, then 
 	 \begin{equation*}
 	 	P(\boldsymbol{u}_{ad}|\delta, \boldsymbol{\lambda}_{ad}) \propto \exp\Big\{\sum_{r \neq a } (\delta+\lambda_{adr})u_{adr}\Big\}.  	 
 	 \end{equation*}
 	 This is straightforward to verify by looking at 
 	 \begin{equation*}
 	 	\begin{aligned}
 	 		&P(u_{adr}=1|\boldsymbol{u}_{ad[-r]}, \delta, \boldsymbol{\lambda}_{ad})
 	 		=\frac{ \exp{(\delta+\lambda_{adr})}}{\exp{(\delta+\lambda_{adr})} + 1}.\end{aligned}\end{equation*}
 	 We denote the logistic-Bernoulli normalizing constant as $Z^{l}(\delta,\boldsymbol{\lambda}_{ad})$, which is defined as 
 	 \begin{equation*}
 	 	Z^{l}(\delta,\boldsymbol{\lambda}_{ad})=\sum_{\boldsymbol{u}_{ad} \in [0,1]^{A}} \exp\Big\{\sum_{r\neq a} (\delta+\lambda_{adr})u_{adr}\Big\}.
 	 \end{equation*}
 	 Now, since 
 	 \begin{equation*}
 	 	\begin{aligned}
 	 		&\exp\Big\{ \mbox{log}\Big(\text{I}(\lVert \boldsymbol{u}_{ad} \rVert_1 > 0)\Big) + \sum_{r \neq a} (\delta+\lambda_{adr})u_{adr} \Big\}\\&= \exp\Big\{  \sum_{r \neq a} (\delta+\lambda_{adr})u_{adr} \Big\},
 	 	\end{aligned}
 	 \end{equation*}
 	 except when $\lVert \boldsymbol{u}_{ad} \rVert_1=0$, we note that 
 	 \begin{equation*}
 	 	\begin{aligned}
 	 		Z(\delta,\boldsymbol{\lambda}_{ad})& = Z^{l}(\delta,\boldsymbol{\lambda}_{ad}) -\exp\Big\{ \sum\limits_{\forall u_{adr}=0}(\delta+\lambda_{adr})u_{adr} \Big\}
 	 		\\& = Z^{l}(\delta,\lambda_{a}^{(d)}) -  1.
 	 	\end{aligned}
 	 \end{equation*}
 	 We can therefore derive a closed form expression for $Z(\delta,\boldsymbol{\lambda}_{ad})$ via a closed form expression for $Z^{l}(\delta,\boldsymbol{\lambda}_{ad})$. This can be done by looking at the probability of the zero vector under the logistic-Bernoulli model:
 	 \begin{equation*}
 	 	\begin{aligned}
 	 		&\frac{\exp\Big\{ \sum\limits_{\forall u_{adr}=0}(\delta+\lambda_{adr})u_{adr} \Big\}}{Z^{l}(\delta,\boldsymbol{\lambda}_{ad})}= \prod_{r \neq a}   \Big(1-\frac{ \exp{(\delta+\lambda_{adr})}}{\exp{(\delta+\lambda_{adr})} + 1}\Big).
 	 	\end{aligned}  
 	 \end{equation*}
 	 Then, we have 
 	 \begin{equation*}
 	 	\begin{aligned}
 	 		& \frac{1}{Z^{l}(\delta,\boldsymbol{\lambda}_{ad})} &= \prod\limits_{r \neq a}\frac{1}{ \exp(\delta+\lambda_{adr})+ 1}.
 	 	\end{aligned}  
 	 \end{equation*}
 	 Finally, the closed form expression for the normalizing constant under the non-empty Gibbs measure is  \begin{equation*}
 	 	\begin{aligned}Z(\delta,\boldsymbol{\lambda}_{ad}) = \prod_{r \neq a } \big(\mbox{exp}\{\delta+\lambda_{adr}\} + 1\big)-1.
 	 	\end{aligned}  
 	 \end{equation*}


\section{Psuedocode for inference} \label{sec: Pseudocode1}
In Section 2, we outlined a Metropolis-within-Gibbs sampling algorithm and each latent variableâ€™s conditional posterior. Algorithm \ref{alg:MCMC} provides the pseudocod for the IPTM's inference. For every outer iteration $o$, we sequentailly resampling the value of each parameter from its conditional posterior given the observed data, hyperparamters, and the current values of the other parameters. For hyperparameter optimization, we fix $n_1=5$. Note that we specify a larger number of inner iterations for ($n_2$, $n_3$,
and $n_4$) such as 5, because those variables require Metropolis-Hastings update which mixes slower than Gibbs update.
\begin{algorithm}[H]
	\caption{Markov Chain Monte Carlo (MCMC)}
		\label{alg:MCMC}
		\begin{algorithmic}
		\STATE \textbf{Input}: data $ \{ (a_d, \boldsymbol{r}_d, t_d,  \boldsymbol{w}_d)\}_{d=1}^D$, \\
		number of interaction patterns and topics $(C, K)$,\\
		hyperparameters $(\alpha, \beta, \boldsymbol{m}, \boldsymbol{\mu}_b, \Sigma_b, \boldsymbol{\mu}_\eta, \Sigma_\eta, {\mu}_\delta,\sigma^2_\delta)$,\\
		number of iterations $(O, n_1, n_2, n_3, n_4)$
		\vskip 0.1in
		\STATE Set initial values
		\FOR{$o=1$  {\bfseries to}  $O$}
		\FOR{$n=1$ {\bfseries to} $n_1$}
			\STATE Optimize $\alpha$ and $\boldsymbol{m}$ using \cite{wallach2008structured}
				\ENDFOR
		\FOR{$d=1$  {\bfseries to}  $D$}
			\FOR{$a \in [A]_{\backslash a_d}$}
				\FOR{$r \in [A]_{\backslash a}$ }
				\STATE	Draw $\boldsymbol{u}_{adr}$ from Equation (13)
					\ENDFOR
						\ENDFOR
		\ENDFOR
			\FOR{$k=1$ {\bfseries to} $K$}
			\STATE Draw $l_k$ from Equation (14)
			\ENDFOR
			
				\FOR{$d=1$  {\bfseries to}  $D$}
			\FOR{$n=1${\bfseries to} $N_d$}
				\STATE Draw $z_{dn}$ from Equation (15)
					\ENDFOR
						\ENDFOR
				\FOR{$n=1$ {\bfseries to} $n_2$}
			\STATE	Draw $\boldsymbol{b}$ and $\delta$ from Equation (16)
				\ENDFOR
			\FOR{$n=1${\bfseries to} $n_3$}
		\STATE	Draw $\boldsymbol{\eta}$ from Equation (17)
			\ENDFOR
		\FOR{$n=1$ {\bfseries to} $n_4$}
		\STATE	Draw $\sigma_\tau^2$ from Equation (17)
			\ENDFOR
 \ENDFOR
	\end{algorithmic}
\end{algorithm}

\section{Psuedocode for posterior predictive checks} \label{sec: Pseudocode2}
      \begin{algorithm}[H]
      	\caption{Generate new data for PPC}
      \begin{algorithmic}
      	\STATE \textbf{Input}: number of new data to generate $R$,\\
      	observed text data $ \{\boldsymbol{w}_d\}_{d=1}^D$,\\
      	estimated latent variables $(\boldsymbol{u},  \boldsymbol{l},\boldsymbol{z}, \boldsymbol{b}, \delta, \boldsymbol{\eta},  \sigma_\tau^2),$\\
      	hyperparameters $(\alpha, \beta, \boldsymbol{m})$,\\
      	number of vocabularies $V$\\
            			\vskip 0.1in
 			
            		\FOR{$r=1$ {\bfseries to}  $R$}
            		       \STATE Initialize $N_{vk}$ and $N_k$ from $\boldsymbol{z}$ and  $\boldsymbol{w}$  
            			\FOR{$d = 1$ {\bfseries to}  $D$} 
      	\IF{$N_{d} > 0$}
      	  		\FOR {$n = 1$  {\bfseries to}  $N_{d}$}
      	  		\STATE Draw	$w_{dn}$ from $P(w_{dn} = v)= \frac{N_{vz_{dn}} +\frac{\beta}{V}}{N_{z_{dn}} + \beta}$\\  			
      	  		\STATE Increment $N_{w_{dn}z_{dn}}$ and $N_{z_{dn}}$ 		
      	  				\ENDFOR
      	  			\ENDIF
			\STATE  Compute $\boldsymbol{x}_{d}$ given $\{(a_{d}, \boldsymbol{r}_{d}, t_{d})\}_{[1:(d-1)]}$
      		\STATE	Draw ($a_{d}$, $\boldsymbol{r}_{d}$, $t_{d}$) following Section 2.3\\
\ENDFOR
\STATE Store every $r^{th}$ new data $\{(a_{d}, \boldsymbol{r}_{d}, t_{d}, \boldsymbol{w}_{d})\}_{d=1}^D$ 
      				\ENDFOR
      		\end{algorithmic}
      \end{algorithm}        
      	 

\bibliography{IPTM}
\bibliographystyle{icml2018}
% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% DELETE THIS PART. DO NOT PLACE CONTENT AFTER THE REFERENCES!
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018. It was modified from a version from Dan Roy in
% 2017, which was based on a version from Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
