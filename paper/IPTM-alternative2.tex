\documentclass[a4paper]{article}
\usepackage{geometry}
\geometry{
	a4paper,
	total={170mm,257mm},
	left=27mm,
	right=30mm,
	top=30mm,
	bottom= 30mm
}
\usepackage{lipsum}
\usepackage{tabu}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{longtable}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{tikz}
\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
		\node[shape=circle,draw,inner sep=0.5pt] (char) {#1};}}
\usetikzlibrary{fit,positioning}
\usepackage{authblk}
\usepackage{natbib}
\usepackage[algo2e]{algorithm2e}
\usepackage{algorithmic}  
\usepackage{algorithm}
\usepackage{comment}
\usepackage{array}% http://ctan.org/pkg/array
\makeatletter
\g@addto@macro{\endtabular}{\rowfont{}}% Clear row font
\makeatother
\newcommand{\rowfonttype}{}% Current row font
\newcommand{\rowfont}[1]{% Set current row font
	\gdef\rowfonttype{#1}#1%
}
\newcolumntype{L}{>{\rowfonttype}l}
\title{A Network Model for Dynamic Textual Communications \\with Application to
	Government Email Corpora}
%\author{Bomin Kim}

\author[1]{Bomin Kim}
\author[3]{Aaron Schein}
\author[1]{Bruce Desmarais}
\author[2,3]{Hanna Wallach}
\affil[1]{Pennsylvania State University}
\affil[2]{Microsoft Research NYC}
\affil[3]{University of Massachusetts Amherst}

\begin{document}
\maketitle
\section{Tie Generating Process}\label{subsec: Tie Generating Process}
We assume the following generative process for each document $d$ in a corpus $D$:
\begin{itemize}
	\item[1.] (Data augmentation) For each sender $i \in \{1,...,A\}$, create a list of receivers denoted by  an indicator vector $J_i$ according to the probability mass function of Gibbs measure:
	\begin{equation}J^{(d)}_{i} = \frac{1}{Z(J^{(d)}_i)}\mbox{exp}\Big(\sum_{j \in \mathcal{A}_{\backslash i}}J^{(d)}_{ij}\lambda^{(d)}_{ij} - \mbox{dlognorm}(\mbox{log}(\sum\limits_{j \in \mathcal{A}_{\backslash i}}J^{(d)}_{ij});0, \delta)\Big), \quad\quad\quad J^{(d)}_{i} \in \mathcal{J}
	\end{equation}
	where $Z(J^{(d)}_i) = \sum\limits_{\forall J^{(d)}_i \in \mathcal{J}}\mbox{exp}\Big(\sum_{j \in \mathcal{A}_{\backslash i}}J^{(d)}_{ij}\lambda^{(d)}_{ij} - \mbox{dlognorm}(\mbox{log}(\sum\limits_{j \in \mathcal{A}_{\backslash i}}J^{(d)}_{ij});0, \delta)\Big)$ is the partition function assuring that the probabilities sum to unity, and $\mathcal{J}$ be the sample space in which $J^{(d)}_i$ may exist (in this case, the sample space consists of all binary $(A-1)$ length vectors). $\delta$ is a positive real-valued parameter that is used as a density penalty.
	\item[2.] For every sender $i \in \mathcal{A}$, generate the time increments \begin{equation}
	\Delta T_{i{J_i}} \sim \mbox{Exp}(\lambda_{i{J_i}}^{(d)}).
	\end{equation}
	\item[3.] Set timestamp, sender, and receivers simultaneously (NOTE: $t^{(0)}=0$):
	\begin{equation}
	\begin{aligned}
	&t^{(d)} = t^{(d-1)}+\mbox{min}(\Delta T_{i{J_i}}),\\
	&i^{(d)} = i_{\mbox{min}(\Delta T_{i{J_i}})}, \\
	&J^{(d)} = J_{i^{(d)}}.
	\end{aligned}
	\end{equation}
\end{itemize}
\section{Inference}
\begin{equation}
\begin{aligned}
&P(\mathcal{J}^{(d)}_{\mbox{a}}, \mathcal{T}^{(d)}_{\mbox{a}}, i^{(d)}_{\mbox{o}}, J^{(d)}_{\mbox{o}}, t^{(d)}_{\mbox{o}} |\mathcal{I}^{(<d)}_{\mbox{o}}, \mathcal{J}^{(<d)}_{\mbox{o}}, \mathcal{T}^{(<d)}_{\mbox{o}}, \mathcal{Z}, \mathcal{C}, \mathcal{B}, \delta)\\&=P\Big(\mbox{latent receivers generation}\Big) \times P\Big(\mbox{latent time generation}\Big)\times P\Big(\mbox{choose the observed}\Big) \\&
=\prod_{i\in \mathcal{A}}\Big(J_{i}^{(d)}\sim \mbox{Gibbs measure}(\{\lambda_{ij}^{(d)}\}_{j=1}^A,\delta)\Big)\times \prod_{i\in \mathcal{A}}\Big(\Delta T^{(d)}_{iJ_i}\sim\mbox{Exp}(\lambda^{(d)}_{iJ_i})\Big) \times \prod_{i\in \mathcal{A}_{\backslash i_o^{(d)}}} P\Big(\Delta T^{(d)}_{i{J_i}} > \Delta T_{i_{o}^{(d)}{J_{o}^{(d)}}}\Big)\\&
=\Big(\prod_{i\in \mathcal{A}} e^{\sum_{j \in \mathcal{A}_{\backslash i}}J^{(d)}_{ij}\lambda^{(d)}_{ij} - \mbox{dlognorm}(\mbox{log}(\sum\limits_{j \in \mathcal{A}_{\backslash i}}J^{(d)}_{ij});0, \delta)}\Big)\times \Big(\prod_{i \in\mathcal{A}}\lambda^{(d)}_{iJ_i}e^{-\Delta T^{(d)}_{iJ_i}\lambda^{(d)}_{iJ_i}}\Big)\times \Big( \prod_{i\in \mathcal{A}_{\backslash i_o^{(d)}}} e^{-\Delta T^{(d)}_{i_o^{(d)}J_o^{(d)}}\lambda^{(d)}_{iJ^{(d)}_{i}}}\Big),
\end{aligned}
\end{equation}
which we can simplify further by integreting out the latent time $\mathcal{T}^{(d)}_{\mbox{a}}=\{\Delta T^{(d)}_{iJ_i}\}_{i \in \mathcal{A}_{\backslash i_o^{(d)}}}$. Then we can simplify Equation (4) as below:
\begin{equation}
\begin{aligned}
&P(\mathcal{J}^{(d)}_{\mbox{a}}, i^{(d)}_{\mbox{o}}, J^{(d)}_{\mbox{o}}, t^{(d)}_{\mbox{o}} |\mathcal{I}^{(<d)}_{\mbox{o}}, \mathcal{J}^{(<d)}_{\mbox{o}}, \mathcal{T}^{(<d)}_{\mbox{o}}, \mathcal{Z}, \mathcal{C}, \mathcal{B}, \delta)\\&=\Big(\prod_{i\in \mathcal{A}} e^{\sum_{j \in \mathcal{A}_{\backslash i}}J^{(d)}_{ij}\lambda^{(d)}_{ij} - \mbox{dlognorm}(\mbox{log}(\sum\limits_{j \in \mathcal{A}_{\backslash i}}J^{(d)}_{ij});0, \delta)}\Big)\times \Big(\lambda^{(d)}_{i_o^{(d)}J_{o}^{(d)}}\Big)\times  \Big(e^{-\Delta T^{(d)}_{i_o^{(d)}J_o^{(d)}}\sum\limits_{i\in \mathcal{A}}\lambda^{(d)}_{iJ^{(d)}_{i}}}\Big),
\end{aligned}
\end{equation}
where this joint distribution can be interpreted as 'probability of latent and observed edges from Gibbs measure $\times$ probability of the observed time comes from Exponential distribution $\times$ probability of all latent time greater than the observed time, given that the latent time also come from Exponential distribution.'
  \subsection{Inference on the augmented data $\mathcal{J}_{\mbox{a}}$}
     Given the observed sender of the document $i_o^{(d)}$, we sample the latent receivers for each sender $i \in \mathcal{A}_{\backslash i_o^{(d)}}$. Here we illustrate how each sender-receiver pair in the document $d$ is updated.\\\newline
       Define ${\mathcal{J}}^{(d)}_{i}$ be the $(A-1)$ length random vector of indicators (0/1) with its realization being $J^{(d)}_{i}$, representing the latent receivers corresponding to the sender $i$ in the document $d$. For each sender $i$, we are going to resample ${J}^{(d)}_{ij}$, which is the $j^{th}$ element of the receiver vector ${J}^{(d)}_{i}$, one at a time with random order. \\
       \begin{equation}
       \begin{aligned}
       &P(\mathcal{J}^{(d)}_{ij} = {J}^{(d)}_{ij}|\mathcal{J}^{(d)}_{i\backslash j}, i^{(d)}_{\mbox{o}}, J^{(d)}_{\mbox{o}}, t^{(d)}_{\mbox{o}}, \mathcal{I}^{(<d)}_{\mbox{o}}, \mathcal{J}^{(<d)}_{\mbox{o}}, \mathcal{T}^{(<d)}_{\mbox{o}}, \mathcal{Z}, \mathcal{C}, \mathcal{B}, \boldsymbol{\delta})\\&\propto P(\mathcal{J}^{(d)}_{i} ={J}^{(d)}_{i}, \mathcal{J}^{(d)}_{i\backslash j}, i^{(d)}_{\mbox{o}}, J^{(d)}_{\mbox{o}}, t^{(d)}_{\mbox{o}}| \mathcal{I}^{(<d)}_{\mbox{o}}, \mathcal{J}^{(<d)}_{\mbox{o}}, \mathcal{T}^{(<d)}_{\mbox{o}}, \mathcal{Z}, \mathcal{C}, \mathcal{B}, \boldsymbol{\delta})
       \\&\propto \mbox{exp}\Big(\sum_{j \in \mathcal{A}_{\backslash i}}J^{(d)}_{ij}\lambda^{(d)}_{ij} - \mbox{dlognorm}(\mbox{log}(\sum\limits_{j \in \mathcal{A}_{\backslash i}}J^{(d)}_{ij});0, \delta)\Big)\times \Big(\lambda^{(d)}_{i_o^{(d)}J_{o}^{(d)}}\Big)\times  \Big(e^{-\Delta T^{(d)}_{i_o^{(d)}J_o^{(d)}}\lambda^{(d)}_{iJ^{(d)}_{i}}}\Big)
       \\& \propto \mbox{exp}\Big(\sum_{j \in \mathcal{A}_{\backslash i}}J^{(d)}_{ij}\lambda^{(d)}_{ij} - \mbox{dlognorm}(\mbox{log}(\sum\limits_{j \in \mathcal{A}_{\backslash i}}J^{(d)}_{ij});0, \delta)\Big)\times \Big(e^{-\Delta T^{(d)}_{i_o^{(d)}J_o^{(d)}}\lambda^{(d)}_{iJ^{(d)}_{i}}}\Big),
       \end{aligned}
       \end{equation}
       where we replace typical use of $(-d)$ to $(<d)$ on the right hand side of the conditional probability, due to the fact that $d^{(th)}$ document only depends on the past documents, not on the future ones.\\ \newline
       To be more specific, since ${J}^{(d)}_{ij}$ could be either 1 or 0, we divide into two cases as below:
       \begin{equation}
       \begin{aligned}
       &P(\mathcal{J}^{(d)}_{ij}=1| \mathcal{J}^{(d)}_{\backslash ij}, i^{(d)}_{\mbox{o}}, J^{(d)}_{\mbox{o}}, t^{(d)}_{\mbox{o}}, \mathcal{I}^{(<d)}_{\mbox{o}}, \mathcal{J}^{(<d)}_{\mbox{o}}, \mathcal{T}^{(<d)}_{\mbox{o}}, \mathcal{Z}, \mathcal{C}, \mathcal{B}, \delta)\\& \propto \mbox{exp}\Big({\sum_{j \in \mathcal{A}_{\backslash i}}J^{(d)}_{i[+j]}\lambda^{(d)}_{ij} - \mbox{dlognorm}(\mbox{log}(\sum\limits_{j \in \mathcal{A}_{\backslash i}}J^{(d)}_{i[+j]});0, \delta)-\Delta T^{(d)}_{i_o^{(d)}J_o^{(d)}}\lambda^{(d)}_{iJ^{(d)}_{ i[+j]}}}\Big),
       \end{aligned}
       \end{equation}
       where $J^{(d)}_{i[+j]}$ meaning that the $j^{th}$ element of $J_{i}^{(d)}$ is fixed as 1. On the other hand, 
       \begin{equation}
       \begin{aligned}
       &P(\mathcal{J}^{(d)}_{ij}=0| \mathcal{J}^{(d)}_{\backslash ij}, i^{(d)}_{\mbox{o}}, J^{(d)}_{\mbox{o}}, t^{(d)}_{\mbox{o}}, \mathcal{I}^{(<d)}_{\mbox{o}}, \mathcal{J}^{(<d)}_{\mbox{o}}, \mathcal{T}^{(<d)}_{\mbox{o}}, \mathcal{Z}, \mathcal{C}, \mathcal{B}, \delta)\\& \propto \mbox{exp}\Big({\sum_{j \in \mathcal{A}_{\backslash i}}J^{(d)}_{i[-j]}\lambda^{(d)}_{ij} - \mbox{dlognorm}(\mbox{log}(\sum\limits_{j \in \mathcal{A}_{\backslash i}}J^{(d)}_{i[-j]});0, \delta)-\Delta T^{(d)}_{i_o^{(d)}J_o^{(d)}}\lambda^{(d)}_{iJ^{(d)}_{ i[-j]}}}\Big),
       \end{aligned}
       \end{equation}
       where $J^{(d)}_{i[-j]}$ meaning similarly that the $j^{th}$ element of $J_{i}^{(d)}$ is fixed as 0. \\\\ \newline Now we can use multinomial sampling using the two probabilities, Equation (7) and Equation (8). Again, we would calculate the probabilities in the log space to prevent from numerical underflow. 
       
 \section{Non-empty Gibbs measure}
 
 The probability that vertex $i$ selects the binary receiver vector of length $n-1$, $J_i^{(d)}$ is given by
 
  $$\text{P}(J_i^{(d)}) = \frac{1}{Z(\delta,\lambda_i^{(d)})} \exp\left[ \ln\left(\text{I}\left( \left(\sum_{j\neq i} J_{ij}\right) > 0 \right)\right) + \sum_{j\neq i} (\delta+\lambda_{ij}^{(d)})J_{ij}^{(d)} \right],$$     where $\delta$ is real-valued intercept that controls the expected value of $J_{ij}^{(d)}$, and $\lambda_{ij}^{(d)}$ is a positive dyad-specific function of the network histories of $i$ and $j$, and any additional attributes included in the model (note, it may fit the data better to include lambda as $\ln\left(\lambda_{ij}^{(d)}\right)$, and $\lambda_{i}^{(d)}$ is the vector of dyadic weights in which $i$ is the sender.   
  
     To use this distribution efficiently, we need to derive a closed-form expression for $\frac{1}{Z(\delta,\lambda_i^{(d)})}$ that does not require brute-force summation over the support of $J_i^{(d)}$. We begin by recognizing that if $J_i^{(d)}$ were drawn via independent Bernoulli distributions in which P($J_{ij}^{(d)}$=1) was given by logit$\left(\delta+\lambda_{ij}^{(d)}\right)$, then $$\text{P}(J_i^{(d)}) \propto \exp\left[  \sum_{j\neq i} (\delta+\lambda_{ij}^{(d)})J_{ij}^{(d)} \right].$$ This is straightforward to verify by looking at $$\text{P}(J_{ij}^{(d)}=1|J_{i,-j})=\frac{ \exp{(\delta+\lambda_{ij})}\exp\left[  \sum_{h\neq i,j} (\delta+\lambda_{ih}^{(d)})J_{ih}^{(d)} \right]}{\exp{(\delta+\lambda_{ij})}\exp\left[  \sum_{h\neq i,j} (\delta+\lambda_{ih}^{(d)})J_{ih}^{(d)} \right] + \exp{(0)}\exp\left[  \sum_{h\neq i,j} (\delta+\lambda_{ih}^{(d)})J_{ih}^{(d)} \right]},$$ $$\text{P}(J_{ij}^{(d)}=1|J_{i,-j})=\frac{ \exp{(\delta+\lambda_{ij})}}{\exp{(\delta+\lambda_{ij})} + 1}.$$ We denote the logistic-Bernoulli normalizing constant as $$Z^{l}(\delta,\lambda_i^{(d)})=\sum_{J_i \in [0,1]^{(n-1)}} \exp\left[  \sum_{j\neq i} (\delta+\lambda_{ij}^{(d)})J_{ij}^{(d)} \right].$$ 
     
     
       
\end{document}

